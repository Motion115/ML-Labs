{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (41753, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Parabéns lojas lannister adorei comprar pela I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>aparelho eficiente. no site a marca do aparelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mas um pouco ,travando...pelo valor ta Boa.\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vendedor confiável, produto ok e entrega antes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                            comment\n",
       "0      5              Recebi bem antes do prazo estipulado.\n",
       "1      5  Parabéns lojas lannister adorei comprar pela I...\n",
       "2      4  aparelho eficiente. no site a marca do aparelh...\n",
       "3      4    Mas um pouco ,travando...pelo valor ta Boa.\\r\\n\n",
       "4      5  Vendedor confiável, produto ok e entrega antes..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reviews = pd.read_csv('olist_order_reviews_dataset.csv')\n",
    "\n",
    "df_comments = reviews.loc[:, ['review_score', 'review_comment_message']]\n",
    "df_comments = df_comments.dropna(subset=['review_comment_message'])\n",
    "df_comments = df_comments.reset_index(drop=True)\n",
    "print(f'Dataset shape: {df_comments.shape}')\n",
    "df_comments.columns = ['score', 'comment']\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Nlp\n",
    "# import nltk\n",
    "# # Wordcloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "# def plot_wordcloud(text, stopwords, mask=None, max_words=200, max_font_size=100,\n",
    "#                    title=None, title_size=40, image_color=False):\n",
    "#\n",
    "#     figure_size = (24, 16)\n",
    "#     wordcloud = WordCloud(background_color='black',\n",
    "#                     stopwords = stopwords,\n",
    "#                     max_words = max_words,\n",
    "#                     max_font_size = max_font_size,\n",
    "#                     random_state = 42,\n",
    "#                     width=1200,\n",
    "#                     height=300,\n",
    "#                     mask = mask)\n",
    "#     wordcloud.generate(str(text))\n",
    "#\n",
    "#     plt.figure(figsize=figure_size)\n",
    "#     plt.imshow(wordcloud);\n",
    "#     plt.title(title, fontdict={'size': title_size, 'color': 'black',\n",
    "#                                   'verticalalignment': 'bottom'})\n",
    "#     plt.axis('off');\n",
    "#     plt.tight_layout()\n",
    "#\n",
    "# stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "# comments = reviews.review_comment_message.values\n",
    "# plot_wordcloud(comments, stopwords, title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "#\n",
    "#\n",
    "# length = [len(text) for text in comments]\n",
    "# num_letters = [len(re.findall(r'[a-zA-Z]', text)) for text in comments]\n",
    "# num_commas = [text.count(',') for text in comments]\n",
    "# num_dots = [text.count('.') for text in comments]\n",
    "#\n",
    "# fig, axis = plt.subplots(1, 2, figsize=(12,4))\n",
    "# pl0 = sns.kdeplot(length, color='navy', label='Review length', ax=axis[0])\n",
    "# pl1 = sns.kdeplot(num_letters, color='orange', label='Number of letters', ax=axis[0])\n",
    "# pl2 = sns.kdeplot(num_dots, color='navy', label='Number of dots', ax=axis[1])\n",
    "# pl3 = sns.kdeplot(num_commas, color='orange', label='Number of commas', ax=axis[1])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Estava faltando apenas um produto, eu recebi hoje , muito obrigada!\r\n",
      "Tudo certo!\r\n",
      "\r\n",
      "Att \r\n",
      "\r\n",
      "Elenice.\n",
      "Processed text: Estava faltando apenas um produto, eu recebi hoje , muito obrigada!  Tudo certo!    Att     Elenice.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def re_breakline(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    return [re.sub('[\\n\\r]', ' ', r) for r in text_list]\n",
    "# Creating a list of comment reviews\n",
    "reviews = list(df_comments['comment'].values)\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_breakline = re_breakline(reviews)\n",
    "df_comments['re_breakline'] = reviews_breakline\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews, reviews_breakline, idx_list=[48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: comprei o produto pela cor ilustrada pelo site da loja americana, no site mostra ser preto http://prntscr.com/jkx7hr quando o produto chegou aqui veio todos com a mesma cor, tabaco http://prntscr.com/\n",
      "Processed text: comprei o produto pela cor ilustrada pelo site da loja americana, no site mostra ser preto  link  quando o produto chegou aqui veio todos com a mesma cor, tabaco  link \n",
      "\n",
      "Original text: Pedi esse: https://www.lannister.com.br/produto/22880118/botox-capilar-selafix-argan-premium-doux-clair-2x1-litro?pfm_carac=doux%20clair&pfm_index=3&pfm_page=search&pfm_pos=grid&pfm_type=search_page%\n",
      "Processed text: Pedi esse:  link \n"
     ]
    }
   ],
   "source": [
    "def re_hiperlinks(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return [re.sub(pattern, ' link ', r) for r in text_list]\n",
    "# Applying RegEx\n",
    "reviews_hiperlinks = re_hiperlinks(reviews_breakline)\n",
    "df_comments['re_hiperlinks'] = reviews_hiperlinks\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_breakline, reviews_hiperlinks, idx_list=[10796, 12782])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Recebi o produto correto, porém o valor do produto na NF ficou a menor, R$ 172,00 sendo que comprei a 219,90.\r\n",
      "O valor do frete calculado foi R$ 18,90 e veio R$ 93,00.\r\n",
      "Gostaria que viesse com correto\n",
      "Processed text: Recebi o produto correto, porém o valor do produto na NF ficou a menor,  dinheiro  sendo que comprei a 219,90.  O valor do frete calculado foi  dinheiro  e veio  dinheiro .  Gostaria que viesse com correto\n",
      "\n",
      "Original text: Paguei $48,00 reais de frete e acabei tendo que buscar o pedido no Centro de Distribuição dos Correios, porém a loja nada tem a ver com o mal serviço prestado pela empresa contrata para entrega.\n",
      "Processed text: Paguei  dinheiro  reais de frete e acabei tendo que buscar o pedido no Centro de Distribuição dos Correios, porém a loja nada tem a ver com o mal serviço prestado pela empresa contrata para entrega.\n",
      "\n",
      "Original text: Infelizmente, para uma entrega em GRU (Região Metropolitana da Grande SP) achei bem \"salgado\" o valor do frete cobrado sobre o preço do produto! Afinal, a mercadoria custou R$26,70 + R$15,11 de frete!\n",
      "Processed text: Infelizmente, para uma entrega em GRU (Região Metropolitana da Grande SP) achei bem \"salgado\" o valor do frete cobrado sobre o preço do produto! Afinal, a mercadoria custou  dinheiro  +  dinheiro  de frete!\n"
     ]
    }
   ],
   "source": [
    "def re_money(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    pattern = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n",
    "    return [re.sub(pattern, ' dinheiro ', r) for r in text_list]\n",
    "# Applying RegEx\n",
    "reviews_money = re_money(reviews_hiperlinks)\n",
    "df_comments['re_money'] = reviews_money\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews, reviews_money, idx_list=[26020, 33297, 32998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Comprei o produto dia 25 de fevereiro e hoje dia 29 de marco não fora entregue na minha residência. Não sei se os correios desse Brasil e péssimo ou foi a própria loja que demorou postar.\n",
      "Processed text: Comprei o produto dia  numero  de fevereiro e hoje dia  numero  de marco não fora entregue na minha residência. Não sei se os correios desse Brasil e péssimo ou foi a própria loja que demorou postar.\n"
     ]
    }
   ],
   "source": [
    "def re_numbers(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    return [re.sub('[0-9]+', ' numero ', r) for r in text_list]\n",
    "# Applying RegEx\n",
    "reviews_numbers = re_numbers(reviews_money)\n",
    "df_comments['re_numbers'] = reviews_numbers\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_money, reviews_numbers, idx_list=[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Meu pedido era para ser entregue até dia  numero / numero / numero , até a presente data ( numero / numero ) a nota fiscal não foi emitida, solicitei várias vezes não obtive retorno, não recomendo esta Loja, nem a lannister!!!!!!\n",
      "Processed text: Meu pedido era para ser entregue até dia  numero / numero / numero , até a presente data ( numero / numero ) a nota fiscal  negação  foi emitida, solicitei várias vezes  negação  obtive retorno,  negação  recomendo esta Loja, nem a lannister!!!!!!\n",
      "\n",
      "Original text: O material é bom, o problema é que a bolsa não fecha, não possui zíper, é como uma sacola. Isso me deixou insatisfeita, pois na foto não dá pra perceber e não há informação ou foto interna sobre isso.\n",
      "Processed text: O material é bom, o problema é que a bolsa  negação  fecha,  negação  possui zíper, é como uma sacola. Isso me deixou insatisfeita, pois na foto  negação  dá pra perceber e  negação  há informação ou foto interna sobre isso.\n",
      "\n",
      "Original text: OEQUIPAMENTO NÃO FUNCIONA. O mini cartao SD nao encaixa e o computador não reconhece quando é conectado com o cabo USB\n",
      "Processed text: OEQUIPAMENTO  negação  FUNCIONA. O mini cartao SD  negação  encaixa e o computador  negação  reconhece quando é conectado com o cabo USB\n",
      "\n",
      "Original text: Cancelei ha tempos, enviaram mesmo assim e nao estornaram os valores\n",
      "Processed text: Cancelei ha tempos, enviaram mesmo assim e  negação  estornaram os valores\n"
     ]
    }
   ],
   "source": [
    "def re_negation(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    return [re.sub('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', ' negação ', r) for r in text_list]\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_negation = re_negation(reviews_numbers)\n",
    "df_comments['re_negation'] = reviews_negation\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_numbers, reviews_negation, idx_list=[4783, 4627, 4856, 4904])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Este foi o pedido  Balde Com  numero  Peças - Blocos De Montar  numero  un -  dinheiro  cada ( negação  FOI ENTREGUE)  Vendido e entregue targaryen  Tapete de Eva Nº Letras  numero  Peças Crianças  numero  un -  dinheiro  (ESTE FOI ENTREG\n",
      "Processed text: Este foi o pedido  Balde Com  numero  Peças   Blocos De Montar  numero  un    dinheiro  cada   negação  FOI ENTREGUE   Vendido e entregue targaryen  Tapete de Eva Nº Letras  numero  Peças Crianças  numero  un    dinheiro   ESTE FOI ENTREG\n",
      "\n",
      "Original text: Cada vez que compro mais fico satisfeita parabéns pela honestidade com seus clientes 👏👏👏👏?\n",
      "Processed text: Cada vez que compro mais fico satisfeita parabéns pela honestidade com seus clientes      \n",
      "\n",
      "Original text: Comprei o produto, paguei no boleto e só recebi metade do produto, anunciaram uma coisa é mandaram outra. Muito insatisfeita 😡😡😡\n",
      "Processed text: Comprei o produto  paguei no boleto e só recebi metade do produto  anunciaram uma coisa é mandaram outra  Muito insatisfeita    \n"
     ]
    }
   ],
   "source": [
    "def re_special_chars(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    return [re.sub('\\W', ' ', r) for r in text_list]\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_special_chars = re_special_chars(reviews_negation)\n",
    "df_comments['re_special_chars'] = reviews_special_chars\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_negation, reviews_special_chars, idx_list=[45, 135, 234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Mas um pouco  travando   pelo valor ta Boa   \n",
      "Processed text: Mas um pouco travando pelo valor ta Boa\n",
      "\n",
      "Original text: Vendedor confiável  produto ok e entrega antes do prazo \n",
      "Processed text: Vendedor confiável produto ok e entrega antes do prazo\n"
     ]
    }
   ],
   "source": [
    "def re_whitespaces(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    white_spaces = [re.sub('\\s+', ' ', r) for r in text_list]\n",
    "    white_spaces_end = [re.sub('[ \\t]+$', '', r) for r in white_spaces]\n",
    "    return white_spaces_end\n",
    "# Applying RegEx\n",
    "reviews_whitespaces = re_whitespaces(reviews_special_chars)\n",
    "df_comments['re_whitespaces'] = reviews_whitespaces\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_special_chars, reviews_whitespaces, idx_list=[3, 4, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total portuguese stopwords in the nltk.corpous module: 207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of some portuguese stopwords\n",
    "pt_stopwords = stopwords.words('portuguese')\n",
    "print(f'Total portuguese stopwords in the nltk.corpous module: {len(pt_stopwords)}')\n",
    "pt_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Recebi bem antes do prazo estipulado\n",
      "Processed text: recebi bem antes prazo estipulado\n",
      "\n",
      "Original text: Este foi o pedido Balde Com numero Peças Blocos De Montar numero un dinheiro cada negação FOI ENTREGUE Vendido e entregue targaryen Tapete de Eva Nº Letras numero Peças Crianças numero un dinheiro ESTE FOI ENTREG\n",
      "Processed text: pedido balde numero peças blocos montar numero un dinheiro cada negação entregue vendido entregue targaryen tapete eva nº letras numero peças crianças numero un dinheiro entreg\n",
      "\n",
      "Original text: O produto negação é bom\n",
      "Processed text: produto negação bom\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to remove the stopwords and to lower the comments\n",
    "def stopwords_removal(text, cached_stopwords=stopwords.words('portuguese')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text: list object where the stopwords will be removed [type: list]\n",
    "    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('portuguese')]\n",
    "    \"\"\"\n",
    "\n",
    "    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]\n",
    "# Removing stopwords and looking at some examples\n",
    "reviews_stopwords = [' '.join(stopwords_removal(review)) for review in reviews_whitespaces]\n",
    "df_comments['stopwords_removed'] = reviews_stopwords\n",
    "\n",
    "print_step_result(reviews_whitespaces, reviews_stopwords, idx_list=[0, 45, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['decepcionou', 'estou', 'estava'], ['acabamento', 'horrível', 'vocês']]\n"
     ]
    }
   ],
   "source": [
    "def CorrectSpelling(text_list):\n",
    "    mistakes_dict = {\n",
    "        'decpcionou': 'decepcionou', 'tô': 'estou', 'to': 'estou',\n",
    "        'q': 'que', 'pq': 'porque', 'mt': 'muito', 'muiiita': 'muita',\n",
    "        'estaav': 'estava', 'acabento': 'acabamento', 'orrivel': 'horrível',\n",
    "        'sertões': 'certos', 'vcs': 'vocês', 'msg': 'mensagem', 'dta': 'data',\n",
    "        'ñ': 'não', 'n': 'não', 'grates': 'grátis', 'testa-lo': 'testar',\n",
    "        'superandoo': 'superando', 'atentimento': 'atendimento',\n",
    "        'cancelacem': 'cancelassem', 'msm': 'mesmo', 'protudo': 'produto',\n",
    "        'decrarar': 'declarar', 'trasporte': 'transporte', 'decpsionei': 'decepcionei',\n",
    "        'empuerada': 'empoeirada', 'recebie': 'recebi', 'superr': 'super',\n",
    "        'nao': 'não', 'mto': 'muito', 'tb': 'também', 'execelente': 'excelente',\n",
    "        'tao': 'tão', 'blz': 'beleza'\n",
    "    }\n",
    "    return [[mistakes_dict[tk] if tk in mistakes_dict else tk for tk in tokens]\n",
    "           for tokens in text_list]\n",
    "text_list = [['decpcionou', 'tô', 'estaav'], ['acabento', 'orrivel', 'vcs']]\n",
    "corrected_text_list = CorrectSpelling(text_list)\n",
    "print(corrected_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['receb bem ant praz estipul', 'parabém loj lannist ador compr internet segur prát parabém tod feliz pásco', 'aparelh efici sit marc aparelh impress numer desinfec cheg outr nom atual marc corret vez aparelh']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35201845 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42800798 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.6947494  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30491281 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.34240858 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.3573852\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.21034983 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33615079 0.         0.         0.         0.27839464\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.71809119 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.34910298 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17292402 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28070932 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36679343 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.71964839 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26495138 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.271608   0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.3080555\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aquel', 'aquil', 'del', 'depo', 'estam', 'estav', 'estej', 'estev', 'estiv', 'estivéss', 'estáv', 'estã', 'fom', 'form', 'foss', 'fôr', 'fôss', 'haj', 'hav', 'houv', 'houvéss', 'hã', 'mesm', 'minh', 'muit', 'noss', 'nã', 'par', 'pel', 'quand', 'sej', 'serã', 'som', 'sã', 'tenh', 'ter', 'terã', 'tev', 'tinh', 'tiv', 'tivéss', 'tính', 'voc', 'éram'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# DataPrep\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Defining regex transformers to be applied\n",
    "regex_transformers = {\n",
    "    'break_line': re_breakline,\n",
    "    'hiperlinks': re_hiperlinks,\n",
    "    'dates': re_dates,\n",
    "    'money': re_money,\n",
    "    'numbers': re_numbers,\n",
    "    'negation': re_negation,\n",
    "    'special_chars': re_special_chars,\n",
    "    'whitespaces': re_whitespaces,\n",
    "    # 'correct_spelling': CorrectSpelling\n",
    "}\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Building the Pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('regex', ApplyRegex(regex_transformers)),\n",
    "    ('stopwords', StopWordsRemoval(stopwords.words('portuguese'))),\n",
    "    ('stemming', StemmingProcess(RSLPStemmer())),\n",
    "    # ('text_features', TextFeatureExtraction(vectorizer)),\n",
    "])\n",
    "from sklearn.cluster import KMeans\n",
    "data = text_pipeline.fit_transform(reviews)\n",
    "print(data[:3])\n",
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "# Defining the vectorizer to extract features from text\n",
    "vectorizer = TfidfVectorizer(max_features=300, stop_words=pt_stopwords,tokenizer=lambda x: [stemmer.stem(word) for word in x.split()])\n",
    "text_feature_extraction = TextFeatureExtraction(vectorizer)\n",
    "ndata = text_feature_extraction.fit_transform(data)\n",
    "print(ndata[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "# # 加载预训练的BERT模型和分词器\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#\n",
    "# # 定义批量大小和步长\n",
    "# batch_size = 16\n",
    "# step_size = 128\n",
    "#\n",
    "# # 对文本进行分词和编码\n",
    "# input_ids = []\n",
    "# attention_masks = []\n",
    "#\n",
    "# for i in range(0, len(reviews), batch_size):\n",
    "#     texts = reviews[i:i+batch_size]\n",
    "#     batch_input_ids = []\n",
    "#     batch_attention_masks = []\n",
    "#\n",
    "#     for text in texts:\n",
    "#         encoded = tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=128,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "#         batch_input_ids.append(encoded['input_ids'])\n",
    "#         batch_attention_masks.append(encoded['attention_mask'])\n",
    "#\n",
    "#     batch_input_ids = torch.cat(batch_input_ids, dim=0)\n",
    "#     batch_attention_masks = torch.cat(batch_attention_masks, dim=0)\n",
    "#\n",
    "#     input_ids.append(batch_input_ids)\n",
    "#     attention_masks.append(batch_attention_masks)\n",
    "#\n",
    "# # 获取BERT嵌入表示\n",
    "# embeddings = []\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     for i in range(len(input_ids)):\n",
    "#         batch_input_ids = input_ids[i]\n",
    "#         batch_attention_masks = attention_masks[i]\n",
    "#         batch_outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "#         batch_embeddings = batch_outputs[0][:, 0, :].numpy()\n",
    "#         embeddings.append(batch_embeddings)\n",
    "#\n",
    "# embeddings = np.concatenate(embeddings, axis=0)\n",
    "#\n",
    "# # 使用K-means算法进行聚类\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(embeddings)\n",
    "#\n",
    "# # 获取聚类标签\n",
    "# labels = kmeans.labels_\n",
    "#\n",
    "# # 计算轮廓系数\n",
    "# silhouette_avg = silhouette_score(embeddings, labels)\n",
    "# print(\"Silhouette Score:\", silhouette_avg)\n",
    "# score_ch = calinski_harabasz_score(embeddings, labels)\n",
    "# print(\"Calinski-Harabasz Score为：\", score_ch)\n",
    "# # 计算Davies-Bouldin Score\n",
    "# score_db = davies_bouldin_score(embeddings, labels)\n",
    "# print(\"Davies-Bouldin Score为：\", score_db)\n",
    "# # 打印每个文本及其所属的聚类标签\n",
    "# for i, text in enumerate(reviews):\n",
    "#     print(\"Text:\", text)\n",
    "#     print(\"Cluster Label:\", labels[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "# from bertopic import BERTopic\n",
    "# import pandas as pd\n",
    "# # 创建BERTopic对象\n",
    "# model = BERTopic(verbose=True, nr_topics=\"auto\", language=\"multilingual\")\n",
    "#\n",
    "# # 将文本转换为列表形式\n",
    "# # docs = reviews.to_list()\n",
    "#\n",
    "# # 拟合模型并进行文本聚类\n",
    "# topics, _ = model.fit_transform(reviews)\n",
    "#\n",
    "# # 打印聚类结果\n",
    "# for doc_idx, topic_idx in enumerate(topics):\n",
    "#     print(f\"文档 {doc_idx} 被分配到主题 {topic_idx}\")\n",
    "#\n",
    "# # 获取主题数\n",
    "# num_topics = model.get_topic_info().shape[0]\n",
    "#\n",
    "# # 计算聚类效果的轮廓系数\n",
    "# silhouette_score = model.get_coherence_per_topic().mean()\n",
    "# print(f\"主题数：{num_topics}\")\n",
    "# print(f\"轮廓系数：{silhouette_score}\")\n",
    "\n",
    "# 初始化BERTopic模型\n",
    "# model = BERTopic(verbose=True, nr_topics=\"auto\", language=\"multilingual\")\n",
    "#\n",
    "# # 将数据集传递给fit_transform方法，进行训练和转换\n",
    "# topics, _ = model.fit_transform(data)\n",
    "#\n",
    "# # 获取聚类标签\n",
    "# labels = model.get_labels()\n",
    "#\n",
    "# # 计算轮廓系数\n",
    "# silhouette_score = silhouette_score(model.umap_embeddings_, labels)\n",
    "# print(\"Silhouette Score:\", silhouette_score)\n",
    "#\n",
    "# # 计算Calinski-Harabasz Score\n",
    "# calinski_harabasz_score = calinski_harabasz_score(model.umap_embeddings_, labels)\n",
    "# print(\"Calinski-Harabasz Score:\", calinski_harabasz_score)\n",
    "#\n",
    "# # 计算Davies-Bouldin Score\n",
    "# davies_bouldin_score = davies_bouldin_score(model.umap_embeddings_, labels)\n",
    "# print(\"Davies-Bouldin Score:\", davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  cluster\n",
      "0                         receb bem ant praz estipul        2\n",
      "1  parabém loj lannist ador compr internet segur ...        4\n",
      "2  aparelh efici sit marc aparelh impress numer d...        4\n",
      "3                               pouc trav val ta boa        4\n",
      "4               vend confi produt ok entreg ant praz        2\n",
      "轮廓系数为： 0.053251820767925624\n",
      "Calinski-Harabasz Score为： 1140.6267095034884\n",
      "Davies-Bouldin Score为： 4.084639800130624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "# 使用KMeans算法对文本数据进行聚类\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(ndata)\n",
    "\n",
    "# 输出聚类结果\n",
    "clusters = kmeans.predict(ndata)\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# 计算聚类效果的轮廓系数\n",
    "score = silhouette_score(ndata, clusters)\n",
    "print(\"轮廓系数为：\", score)\n",
    "# 计算Calinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(ndata, clusters)\n",
    "print(\"Calinski-Harabasz Score为：\", score_ch)\n",
    "# 计算Davies-Bouldin Score\n",
    "score_db = davies_bouldin_score(ndata, clusters)\n",
    "print(\"Davies-Bouldin Score为：\", score_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      2\u001b[0m embedder \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m data_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      5\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(data_embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:434\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    425\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m         output_attentions,\n\u001b[0;32m    433\u001b[0m     )\n\u001b[1;32m--> 434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:384\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 384\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    386\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "data_embeddings = embedder.encode(data)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(data_embeddings)\n",
    "clusters = kmeans.predict(data_embeddings)\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# 计算聚类效果的轮廓系数\n",
    "score = silhouette_score(data_embeddings, clusters)\n",
    "print(\"轮廓系数为：\", score)\n",
    "# 计算Calinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(data_embeddings, clusters)\n",
    "print(\"Calinski-Harabasz Score为：\", score_ch)\n",
    "# 计算Davies-Bouldin Score\n",
    "score_db = davies_bouldin_score(data_embeddings, clusters)\n",
    "print(\"Davies-Bouldin Score为：\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# 使用DBSCAN算法对文本数据进行聚类\n",
    "dbscan_cluster = DBSCAN(eps=0.5, min_samples=2)\n",
    "dbscan_cluster.fit(data_embeddings)\n",
    "\n",
    "# 输出聚类结果\n",
    "clusters = dbscan_cluster.labels_\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters[:10])\n",
    "#02\n",
    "# 计算聚类效果的轮廓系数\n",
    "score = silhouette_score(data_embeddings, clusters)\n",
    "print(\"轮廓系数为：\", score)\n",
    "# 计算Calinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(data_embeddings, clusters)\n",
    "print(\"Calinski-Harabasz Score为：\", score_ch)\n",
    "# 计算Davies-Bouldin Score\n",
    "score_db = davies_bouldin_score(data_embeddings, clusters)\n",
    "print(\"Davies-Bouldin Score为：\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# 使用DBSCAN算法对文本数据进行聚类\n",
    "dbscan_cluster = DBSCAN(eps=0.5, min_samples=2)\n",
    "dbscan_cluster.fit(ndata)\n",
    "\n",
    "# 输出聚类结果\n",
    "clusters = dbscan_cluster.labels_\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters[:10])\n",
    "#02\n",
    "# 计算聚类效果的轮廓系数\n",
    "score = silhouette_score(ndata, clusters)\n",
    "print(\"轮廓系数为：\", score)\n",
    "# 计算Calinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(ndata, clusters)\n",
    "print(\"Calinski-Harabasz Score为：\", score_ch)\n",
    "# 计算Davies-Bouldin Score\n",
    "score_db = davies_bouldin_score(ndata, clusters)\n",
    "print(\"Davies-Bouldin Score为：\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "# 使用高斯混合模型聚类算法对文本数据进行聚类\n",
    "gmm_cluster = GaussianMixture(n_components=5, random_state=42)\n",
    "gmm_cluster.fit(ndata)\n",
    "\n",
    "# 输出聚类结果\n",
    "clusters = gmm_cluster.predict(ndata)\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# 计算聚类效果的轮廓系数\n",
    "score = silhouette_score(ndata, clusters)\n",
    "print(\"轮廓系数为：\", score)\n",
    "# 计算Calinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(ndata, clusters)\n",
    "print(\"Calinski-Harabasz Score为：\", score_ch)\n",
    "# 计算Davies-Bouldin Score\n",
    "score_db = davies_bouldin_score(ndata, clusters)\n",
    "print(\"Davies-Bouldin Score为：\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "# 使用高斯混合模型聚类算法对文本数据进行聚类\n",
    "gmm_cluster = GaussianMixture(n_components=5, random_state=42)\n",
    "gmm_cluster.fit(data_embeddings)\n",
    "\n",
    "# 输出聚类结果\n",
    "clusters = gmm_cluster.predict(data_embeddings)\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# 计算聚类效果的轮廓系数\n",
    "score = silhouette_score(data_embeddings, clusters)\n",
    "print(\"轮廓系数为：\", score)\n",
    "# 计算Calinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(data_embeddings, clusters)\n",
    "print(\"Calinski-Harabasz Score为：\", score_ch)\n",
    "# 计算Davies-Bouldin Score\n",
    "score_db = davies_bouldin_score(data_embeddings, clusters)\n",
    "print(\"Davies-Bouldin Score为：\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化HDBSCAN模型\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric='euclidean')\n",
    "\n",
    "# 训练模型\n",
    "clusterer.fit(ndata)\n",
    "# 输出聚类结果\n",
    "clusters = clusterer.predict(ndata)\n",
    "\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# 计算聚类效果的轮廓系数\n",
    "score = silhouette_score(ndata, clusters)\n",
    "print(\"轮廓系数为：\", score)\n",
    "# 计算Calinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(ndata, clusters)\n",
    "print(\"Calinski-Harabasz Score为：\", score_ch)\n",
    "# 计算Davies-Bouldin Score\n",
    "score_db = davies_bouldin_score(ndata, clusters)\n",
    "print(\"Davies-Bouldin Score为：\", score_db)\n",
    "# 可视化聚类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced = TruncatedSVD(n_components=50, random_state=0).fit_transform(ndata)\n",
    "X_embedded = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(X_reduced)\n",
    "plt.scatter(X_embedded[:,0],X_embedded[:,1], c=clusterer.labels_, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "#\n",
    "# from sklearn.feature_extraction import text\n",
    "# from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "# my_stop_words = text.ENGLISH_STOP_WORDS.union([\"portuguese\"])\n",
    "# # 创建Snowball词干提取器\n",
    "# stemmer = SnowballStemmer(\"portuguese\")\n",
    "#\n",
    "# # 创建TF-IDF向量化器，并指定使用词干提取器\n",
    "# vectorizer = TfidfVectorizer(stop_words=my_stop_words, tokenizer=lambda x: [stemmer.stem(word) for word in x.split()])\n",
    "#\n",
    "# # 对文本数据进行向量化\n",
    "# tfidf = vectorizer.fit_transform(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
