{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (41753, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Parab√©ns lojas lannister adorei comprar pela I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>aparelho eficiente. no site a marca do aparelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mas um pouco ,travando...pelo valor ta Boa.\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vendedor confi√°vel, produto ok e entrega antes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                            comment\n",
       "0      5              Recebi bem antes do prazo estipulado.\n",
       "1      5  Parab√©ns lojas lannister adorei comprar pela I...\n",
       "2      4  aparelho eficiente. no site a marca do aparelh...\n",
       "3      4    Mas um pouco ,travando...pelo valor ta Boa.\\r\\n\n",
       "4      5  Vendedor confi√°vel, produto ok e entrega antes..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reviews = pd.read_csv('olist_order_reviews_dataset.csv')\n",
    "\n",
    "df_comments = reviews.loc[:, ['review_score', 'review_comment_message']]\n",
    "df_comments = df_comments.dropna(subset=['review_comment_message'])\n",
    "df_comments = df_comments.reset_index(drop=True)\n",
    "print(f'Dataset shape: {df_comments.shape}')\n",
    "df_comments.columns = ['score', 'comment']\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Nlp\n",
    "# import nltk\n",
    "# # Wordcloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "# def plot_wordcloud(text, stopwords, mask=None, max_words=200, max_font_size=100,\n",
    "#                    title=None, title_size=40, image_color=False):\n",
    "#\n",
    "#     figure_size = (24, 16)\n",
    "#     wordcloud = WordCloud(background_color='black',\n",
    "#                     stopwords = stopwords,\n",
    "#                     max_words = max_words,\n",
    "#                     max_font_size = max_font_size,\n",
    "#                     random_state = 42,\n",
    "#                     width=1200,\n",
    "#                     height=300,\n",
    "#                     mask = mask)\n",
    "#     wordcloud.generate(str(text))\n",
    "#\n",
    "#     plt.figure(figsize=figure_size)\n",
    "#     plt.imshow(wordcloud);\n",
    "#     plt.title(title, fontdict={'size': title_size, 'color': 'black',\n",
    "#                                   'verticalalignment': 'bottom'})\n",
    "#     plt.axis('off');\n",
    "#     plt.tight_layout()\n",
    "#\n",
    "# stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "# comments = reviews.review_comment_message.values\n",
    "# plot_wordcloud(comments, stopwords, title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "#\n",
    "#\n",
    "# length = [len(text) for text in comments]\n",
    "# num_letters = [len(re.findall(r'[a-zA-Z]', text)) for text in comments]\n",
    "# num_commas = [text.count(',') for text in comments]\n",
    "# num_dots = [text.count('.') for text in comments]\n",
    "#\n",
    "# fig, axis = plt.subplots(1, 2, figsize=(12,4))\n",
    "# pl0 = sns.kdeplot(length, color='navy', label='Review length', ax=axis[0])\n",
    "# pl1 = sns.kdeplot(num_letters, color='orange', label='Number of letters', ax=axis[0])\n",
    "# pl2 = sns.kdeplot(num_dots, color='navy', label='Number of dots', ax=axis[1])\n",
    "# pl3 = sns.kdeplot(num_commas, color='orange', label='Number of commas', ax=axis[1])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Estava faltando apenas um produto, eu recebi hoje , muito obrigada!\r\n",
      "Tudo certo!\r\n",
      "\r\n",
      "Att \r\n",
      "\r\n",
      "Elenice.\n",
      "Processed text: Estava faltando apenas um produto, eu recebi hoje , muito obrigada!  Tudo certo!    Att     Elenice.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def re_breakline(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    return [re.sub('[\\n\\r]', ' ', r) for r in text_list]\n",
    "# Creating a list of comment reviews\n",
    "reviews = list(df_comments['comment'].values)\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_breakline = re_breakline(reviews)\n",
    "df_comments['re_breakline'] = reviews_breakline\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews, reviews_breakline, idx_list=[48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: comprei o produto pela cor ilustrada pelo site da loja americana, no site mostra ser preto http://prntscr.com/jkx7hr quando o produto chegou aqui veio todos com a mesma cor, tabaco http://prntscr.com/\n",
      "Processed text: comprei o produto pela cor ilustrada pelo site da loja americana, no site mostra ser preto  link  quando o produto chegou aqui veio todos com a mesma cor, tabaco  link \n",
      "\n",
      "Original text: Pedi esse: https://www.lannister.com.br/produto/22880118/botox-capilar-selafix-argan-premium-doux-clair-2x1-litro?pfm_carac=doux%20clair&pfm_index=3&pfm_page=search&pfm_pos=grid&pfm_type=search_page%\n",
      "Processed text: Pedi esse:  link \n"
     ]
    }
   ],
   "source": [
    "def re_hiperlinks(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return [re.sub(pattern, ' link ', r) for r in text_list]\n",
    "# Applying RegEx\n",
    "reviews_hiperlinks = re_hiperlinks(reviews_breakline)\n",
    "df_comments['re_hiperlinks'] = reviews_hiperlinks\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_breakline, reviews_hiperlinks, idx_list=[10796, 12782])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Recebi o produto correto, por√©m o valor do produto na NF ficou a menor, R$ 172,00 sendo que comprei a 219,90.\r\n",
      "O valor do frete calculado foi R$ 18,90 e veio R$ 93,00.\r\n",
      "Gostaria que viesse com correto\n",
      "Processed text: Recebi o produto correto, por√©m o valor do produto na NF ficou a menor,  dinheiro  sendo que comprei a 219,90.  O valor do frete calculado foi  dinheiro  e veio  dinheiro .  Gostaria que viesse com correto\n",
      "\n",
      "Original text: Paguei $48,00 reais de frete e acabei tendo que buscar o pedido no Centro de Distribui√ß√£o dos Correios, por√©m a loja nada tem a ver com o mal servi√ßo prestado pela empresa contrata para entrega.\n",
      "Processed text: Paguei  dinheiro  reais de frete e acabei tendo que buscar o pedido no Centro de Distribui√ß√£o dos Correios, por√©m a loja nada tem a ver com o mal servi√ßo prestado pela empresa contrata para entrega.\n",
      "\n",
      "Original text: Infelizmente, para uma entrega em GRU (Regi√£o Metropolitana da Grande SP) achei bem \"salgado\" o valor do frete cobrado sobre o pre√ßo do produto! Afinal, a mercadoria custou R$26,70 + R$15,11 de frete!\n",
      "Processed text: Infelizmente, para uma entrega em GRU (Regi√£o Metropolitana da Grande SP) achei bem \"salgado\" o valor do frete cobrado sobre o pre√ßo do produto! Afinal, a mercadoria custou  dinheiro  +  dinheiro  de frete!\n"
     ]
    }
   ],
   "source": [
    "def re_money(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    pattern = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n",
    "    return [re.sub(pattern, ' dinheiro ', r) for r in text_list]\n",
    "# Applying RegEx\n",
    "reviews_money = re_money(reviews_hiperlinks)\n",
    "df_comments['re_money'] = reviews_money\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews, reviews_money, idx_list=[26020, 33297, 32998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Comprei o produto dia 25 de fevereiro e hoje dia 29 de marco n√£o fora entregue na minha resid√™ncia. N√£o sei se os correios desse Brasil e p√©ssimo ou foi a pr√≥pria loja que demorou postar.\n",
      "Processed text: Comprei o produto dia  numero  de fevereiro e hoje dia  numero  de marco n√£o fora entregue na minha resid√™ncia. N√£o sei se os correios desse Brasil e p√©ssimo ou foi a pr√≥pria loja que demorou postar.\n"
     ]
    }
   ],
   "source": [
    "def re_numbers(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    return [re.sub('[0-9]+', ' numero ', r) for r in text_list]\n",
    "# Applying RegEx\n",
    "reviews_numbers = re_numbers(reviews_money)\n",
    "df_comments['re_numbers'] = reviews_numbers\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_money, reviews_numbers, idx_list=[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Meu pedido era para ser entregue at√© dia  numero / numero / numero , at√© a presente data ( numero / numero ) a nota fiscal n√£o foi emitida, solicitei v√°rias vezes n√£o obtive retorno, n√£o recomendo esta Loja, nem a lannister!!!!!!\n",
      "Processed text: Meu pedido era para ser entregue at√© dia  numero / numero / numero , at√© a presente data ( numero / numero ) a nota fiscal  nega√ß√£o  foi emitida, solicitei v√°rias vezes  nega√ß√£o  obtive retorno,  nega√ß√£o  recomendo esta Loja, nem a lannister!!!!!!\n",
      "\n",
      "Original text: O material √© bom, o problema √© que a bolsa n√£o fecha, n√£o possui z√≠per, √© como uma sacola. Isso me deixou insatisfeita, pois na foto n√£o d√° pra perceber e n√£o h√° informa√ß√£o ou foto interna sobre isso.\n",
      "Processed text: O material √© bom, o problema √© que a bolsa  nega√ß√£o  fecha,  nega√ß√£o  possui z√≠per, √© como uma sacola. Isso me deixou insatisfeita, pois na foto  nega√ß√£o  d√° pra perceber e  nega√ß√£o  h√° informa√ß√£o ou foto interna sobre isso.\n",
      "\n",
      "Original text: OEQUIPAMENTO N√ÉO FUNCIONA. O mini cartao SD nao encaixa e o computador n√£o reconhece quando √© conectado com o cabo USB\n",
      "Processed text: OEQUIPAMENTO  nega√ß√£o  FUNCIONA. O mini cartao SD  nega√ß√£o  encaixa e o computador  nega√ß√£o  reconhece quando √© conectado com o cabo USB\n",
      "\n",
      "Original text: Cancelei ha tempos, enviaram mesmo assim e nao estornaram os valores\n",
      "Processed text: Cancelei ha tempos, enviaram mesmo assim e  nega√ß√£o  estornaram os valores\n"
     ]
    }
   ],
   "source": [
    "def re_negation(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    return [re.sub('([nN][√£√ÉaA][oO]|[√±√ë]| [nN] )', ' nega√ß√£o ', r) for r in text_list]\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_negation = re_negation(reviews_numbers)\n",
    "df_comments['re_negation'] = reviews_negation\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_numbers, reviews_negation, idx_list=[4783, 4627, 4856, 4904])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Este foi o pedido  Balde Com  numero  Pe√ßas - Blocos De Montar  numero  un -  dinheiro  cada ( nega√ß√£o  FOI ENTREGUE)  Vendido e entregue targaryen  Tapete de Eva N¬∫ Letras  numero  Pe√ßas Crian√ßas  numero  un -  dinheiro  (ESTE FOI ENTREG\n",
      "Processed text: Este foi o pedido  Balde Com  numero  Pe√ßas   Blocos De Montar  numero  un    dinheiro  cada   nega√ß√£o  FOI ENTREGUE   Vendido e entregue targaryen  Tapete de Eva N¬∫ Letras  numero  Pe√ßas Crian√ßas  numero  un    dinheiro   ESTE FOI ENTREG\n",
      "\n",
      "Original text: Cada vez que compro mais fico satisfeita parab√©ns pela honestidade com seus clientes üëèüëèüëèüëè?\n",
      "Processed text: Cada vez que compro mais fico satisfeita parab√©ns pela honestidade com seus clientes      \n",
      "\n",
      "Original text: Comprei o produto, paguei no boleto e s√≥ recebi metade do produto, anunciaram uma coisa √© mandaram outra. Muito insatisfeita üò°üò°üò°\n",
      "Processed text: Comprei o produto  paguei no boleto e s√≥ recebi metade do produto  anunciaram uma coisa √© mandaram outra  Muito insatisfeita    \n"
     ]
    }
   ],
   "source": [
    "def re_special_chars(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    return [re.sub('\\W', ' ', r) for r in text_list]\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_special_chars = re_special_chars(reviews_negation)\n",
    "df_comments['re_special_chars'] = reviews_special_chars\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_negation, reviews_special_chars, idx_list=[45, 135, 234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Mas um pouco  travando   pelo valor ta Boa   \n",
      "Processed text: Mas um pouco travando pelo valor ta Boa\n",
      "\n",
      "Original text: Vendedor confi√°vel  produto ok e entrega antes do prazo \n",
      "Processed text: Vendedor confi√°vel produto ok e entrega antes do prazo\n"
     ]
    }
   ],
   "source": [
    "def re_whitespaces(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    white_spaces = [re.sub('\\s+', ' ', r) for r in text_list]\n",
    "    white_spaces_end = [re.sub('[ \\t]+$', '', r) for r in white_spaces]\n",
    "    return white_spaces_end\n",
    "# Applying RegEx\n",
    "reviews_whitespaces = re_whitespaces(reviews_special_chars)\n",
    "df_comments['re_whitespaces'] = reviews_whitespaces\n",
    "\n",
    "# Verifying results\n",
    "print_step_result(reviews_special_chars, reviews_whitespaces, idx_list=[3, 4, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total portuguese stopwords in the nltk.corpous module: 207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " '√†',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of some portuguese stopwords\n",
    "pt_stopwords = stopwords.words('portuguese')\n",
    "print(f'Total portuguese stopwords in the nltk.corpous module: {len(pt_stopwords)}')\n",
    "pt_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: Recebi bem antes do prazo estipulado\n",
      "Processed text: recebi bem antes prazo estipulado\n",
      "\n",
      "Original text: Este foi o pedido Balde Com numero Pe√ßas Blocos De Montar numero un dinheiro cada nega√ß√£o FOI ENTREGUE Vendido e entregue targaryen Tapete de Eva N¬∫ Letras numero Pe√ßas Crian√ßas numero un dinheiro ESTE FOI ENTREG\n",
      "Processed text: pedido balde numero pe√ßas blocos montar numero un dinheiro cada nega√ß√£o entregue vendido entregue targaryen tapete eva n¬∫ letras numero pe√ßas crian√ßas numero un dinheiro entreg\n",
      "\n",
      "Original text: O produto nega√ß√£o √© bom\n",
      "Processed text: produto nega√ß√£o bom\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to remove the stopwords and to lower the comments\n",
    "def stopwords_removal(text, cached_stopwords=stopwords.words('portuguese')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text: list object where the stopwords will be removed [type: list]\n",
    "    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('portuguese')]\n",
    "    \"\"\"\n",
    "\n",
    "    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]\n",
    "# Removing stopwords and looking at some examples\n",
    "reviews_stopwords = [' '.join(stopwords_removal(review)) for review in reviews_whitespaces]\n",
    "df_comments['stopwords_removed'] = reviews_stopwords\n",
    "\n",
    "print_step_result(reviews_whitespaces, reviews_stopwords, idx_list=[0, 45, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['decepcionou', 'estou', 'estava'], ['acabamento', 'horr√≠vel', 'voc√™s']]\n"
     ]
    }
   ],
   "source": [
    "def CorrectSpelling(text_list):\n",
    "    mistakes_dict = {\n",
    "        'decpcionou': 'decepcionou', 't√¥': 'estou', 'to': 'estou',\n",
    "        'q': 'que', 'pq': 'porque', 'mt': 'muito', 'muiiita': 'muita',\n",
    "        'estaav': 'estava', 'acabento': 'acabamento', 'orrivel': 'horr√≠vel',\n",
    "        'sert√µes': 'certos', 'vcs': 'voc√™s', 'msg': 'mensagem', 'dta': 'data',\n",
    "        '√±': 'n√£o', 'n': 'n√£o', 'grates': 'gr√°tis', 'testa-lo': 'testar',\n",
    "        'superandoo': 'superando', 'atentimento': 'atendimento',\n",
    "        'cancelacem': 'cancelassem', 'msm': 'mesmo', 'protudo': 'produto',\n",
    "        'decrarar': 'declarar', 'trasporte': 'transporte', 'decpsionei': 'decepcionei',\n",
    "        'empuerada': 'empoeirada', 'recebie': 'recebi', 'superr': 'super',\n",
    "        'nao': 'n√£o', 'mto': 'muito', 'tb': 'tamb√©m', 'execelente': 'excelente',\n",
    "        'tao': 't√£o', 'blz': 'beleza'\n",
    "    }\n",
    "    return [[mistakes_dict[tk] if tk in mistakes_dict else tk for tk in tokens]\n",
    "           for tokens in text_list]\n",
    "text_list = [['decpcionou', 't√¥', 'estaav'], ['acabento', 'orrivel', 'vcs']]\n",
    "corrected_text_list = CorrectSpelling(text_list)\n",
    "print(corrected_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['receb bem ant praz estipul', 'parab√©m loj lannist ador compr internet segur pr√°t parab√©m tod feliz p√°sco', 'aparelh efici sit marc aparelh impress numer desinfec cheg outr nom atual marc corret vez aparelh']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35201845 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42800798 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.6947494  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30491281 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.34240858 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.3573852\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.21034983 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33615079 0.         0.         0.         0.27839464\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.71809119 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.34910298 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17292402 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28070932 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36679343 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.71964839 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26495138 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.271608   0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.3080555\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aquel', 'aquil', 'del', 'depo', 'estam', 'estav', 'estej', 'estev', 'estiv', 'estiv√©ss', 'est√°v', 'est√£', 'fom', 'form', 'foss', 'f√¥r', 'f√¥ss', 'haj', 'hav', 'houv', 'houv√©ss', 'h√£', 'mesm', 'minh', 'muit', 'noss', 'n√£', 'par', 'pel', 'quand', 'sej', 'ser√£', 'som', 's√£', 'tenh', 'ter', 'ter√£', 'tev', 'tinh', 'tiv', 'tiv√©ss', 't√≠nh', 'voc', '√©ram'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# DataPrep\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Defining regex transformers to be applied\n",
    "regex_transformers = {\n",
    "    'break_line': re_breakline,\n",
    "    'hiperlinks': re_hiperlinks,\n",
    "    'dates': re_dates,\n",
    "    'money': re_money,\n",
    "    'numbers': re_numbers,\n",
    "    'negation': re_negation,\n",
    "    'special_chars': re_special_chars,\n",
    "    'whitespaces': re_whitespaces,\n",
    "    # 'correct_spelling': CorrectSpelling\n",
    "}\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Building the Pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('regex', ApplyRegex(regex_transformers)),\n",
    "    ('stopwords', StopWordsRemoval(stopwords.words('portuguese'))),\n",
    "    ('stemming', StemmingProcess(RSLPStemmer())),\n",
    "    # ('text_features', TextFeatureExtraction(vectorizer)),\n",
    "])\n",
    "from sklearn.cluster import KMeans\n",
    "data = text_pipeline.fit_transform(reviews)\n",
    "print(data[:3])\n",
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "# Defining the vectorizer to extract features from text\n",
    "vectorizer = TfidfVectorizer(max_features=300, stop_words=pt_stopwords,tokenizer=lambda x: [stemmer.stem(word) for word in x.split()])\n",
    "text_feature_extraction = TextFeatureExtraction(vectorizer)\n",
    "ndata = text_feature_extraction.fit_transform(data)\n",
    "print(ndata[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "# # Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑBERTÊ®°ÂûãÂíåÂàÜËØçÂô®\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#\n",
    "# # ÂÆö‰πâÊâπÈáèÂ§ßÂ∞èÂíåÊ≠•Èïø\n",
    "# batch_size = 16\n",
    "# step_size = 128\n",
    "#\n",
    "# # ÂØπÊñáÊú¨ËøõË°åÂàÜËØçÂíåÁºñÁ†Å\n",
    "# input_ids = []\n",
    "# attention_masks = []\n",
    "#\n",
    "# for i in range(0, len(reviews), batch_size):\n",
    "#     texts = reviews[i:i+batch_size]\n",
    "#     batch_input_ids = []\n",
    "#     batch_attention_masks = []\n",
    "#\n",
    "#     for text in texts:\n",
    "#         encoded = tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=128,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "#         batch_input_ids.append(encoded['input_ids'])\n",
    "#         batch_attention_masks.append(encoded['attention_mask'])\n",
    "#\n",
    "#     batch_input_ids = torch.cat(batch_input_ids, dim=0)\n",
    "#     batch_attention_masks = torch.cat(batch_attention_masks, dim=0)\n",
    "#\n",
    "#     input_ids.append(batch_input_ids)\n",
    "#     attention_masks.append(batch_attention_masks)\n",
    "#\n",
    "# # Ëé∑ÂèñBERTÂµåÂÖ•Ë°®Á§∫\n",
    "# embeddings = []\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     for i in range(len(input_ids)):\n",
    "#         batch_input_ids = input_ids[i]\n",
    "#         batch_attention_masks = attention_masks[i]\n",
    "#         batch_outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "#         batch_embeddings = batch_outputs[0][:, 0, :].numpy()\n",
    "#         embeddings.append(batch_embeddings)\n",
    "#\n",
    "# embeddings = np.concatenate(embeddings, axis=0)\n",
    "#\n",
    "# # ‰ΩøÁî®K-meansÁÆóÊ≥ïËøõË°åËÅöÁ±ª\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(embeddings)\n",
    "#\n",
    "# # Ëé∑ÂèñËÅöÁ±ªÊ†áÁ≠æ\n",
    "# labels = kmeans.labels_\n",
    "#\n",
    "# # ËÆ°ÁÆóËΩÆÂªìÁ≥ªÊï∞\n",
    "# silhouette_avg = silhouette_score(embeddings, labels)\n",
    "# print(\"Silhouette Score:\", silhouette_avg)\n",
    "# score_ch = calinski_harabasz_score(embeddings, labels)\n",
    "# print(\"Calinski-Harabasz Score‰∏∫Ôºö\", score_ch)\n",
    "# # ËÆ°ÁÆóDavies-Bouldin Score\n",
    "# score_db = davies_bouldin_score(embeddings, labels)\n",
    "# print(\"Davies-Bouldin Score‰∏∫Ôºö\", score_db)\n",
    "# # ÊâìÂç∞ÊØè‰∏™ÊñáÊú¨ÂèäÂÖ∂ÊâÄÂ±ûÁöÑËÅöÁ±ªÊ†áÁ≠æ\n",
    "# for i, text in enumerate(reviews):\n",
    "#     print(\"Text:\", text)\n",
    "#     print(\"Cluster Label:\", labels[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "# from bertopic import BERTopic\n",
    "# import pandas as pd\n",
    "# # ÂàõÂª∫BERTopicÂØπË±°\n",
    "# model = BERTopic(verbose=True, nr_topics=\"auto\", language=\"multilingual\")\n",
    "#\n",
    "# # Â∞ÜÊñáÊú¨ËΩ¨Êç¢‰∏∫ÂàóË°®ÂΩ¢Âºè\n",
    "# # docs = reviews.to_list()\n",
    "#\n",
    "# # ÊãüÂêàÊ®°ÂûãÂπ∂ËøõË°åÊñáÊú¨ËÅöÁ±ª\n",
    "# topics, _ = model.fit_transform(reviews)\n",
    "#\n",
    "# # ÊâìÂç∞ËÅöÁ±ªÁªìÊûú\n",
    "# for doc_idx, topic_idx in enumerate(topics):\n",
    "#     print(f\"ÊñáÊ°£ {doc_idx} Ë¢´ÂàÜÈÖçÂà∞‰∏ªÈ¢ò {topic_idx}\")\n",
    "#\n",
    "# # Ëé∑Âèñ‰∏ªÈ¢òÊï∞\n",
    "# num_topics = model.get_topic_info().shape[0]\n",
    "#\n",
    "# # ËÆ°ÁÆóËÅöÁ±ªÊïàÊûúÁöÑËΩÆÂªìÁ≥ªÊï∞\n",
    "# silhouette_score = model.get_coherence_per_topic().mean()\n",
    "# print(f\"‰∏ªÈ¢òÊï∞Ôºö{num_topics}\")\n",
    "# print(f\"ËΩÆÂªìÁ≥ªÊï∞Ôºö{silhouette_score}\")\n",
    "\n",
    "# ÂàùÂßãÂåñBERTopicÊ®°Âûã\n",
    "# model = BERTopic(verbose=True, nr_topics=\"auto\", language=\"multilingual\")\n",
    "#\n",
    "# # Â∞ÜÊï∞ÊçÆÈõÜ‰º†ÈÄíÁªôfit_transformÊñπÊ≥ïÔºåËøõË°åËÆ≠ÁªÉÂíåËΩ¨Êç¢\n",
    "# topics, _ = model.fit_transform(data)\n",
    "#\n",
    "# # Ëé∑ÂèñËÅöÁ±ªÊ†áÁ≠æ\n",
    "# labels = model.get_labels()\n",
    "#\n",
    "# # ËÆ°ÁÆóËΩÆÂªìÁ≥ªÊï∞\n",
    "# silhouette_score = silhouette_score(model.umap_embeddings_, labels)\n",
    "# print(\"Silhouette Score:\", silhouette_score)\n",
    "#\n",
    "# # ËÆ°ÁÆóCalinski-Harabasz Score\n",
    "# calinski_harabasz_score = calinski_harabasz_score(model.umap_embeddings_, labels)\n",
    "# print(\"Calinski-Harabasz Score:\", calinski_harabasz_score)\n",
    "#\n",
    "# # ËÆ°ÁÆóDavies-Bouldin Score\n",
    "# davies_bouldin_score = davies_bouldin_score(model.umap_embeddings_, labels)\n",
    "# print(\"Davies-Bouldin Score:\", davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  cluster\n",
      "0                         receb bem ant praz estipul        2\n",
      "1  parab√©m loj lannist ador compr internet segur ...        4\n",
      "2  aparelh efici sit marc aparelh impress numer d...        4\n",
      "3                               pouc trav val ta boa        4\n",
      "4               vend confi produt ok entreg ant praz        2\n",
      "ËΩÆÂªìÁ≥ªÊï∞‰∏∫Ôºö 0.053251820767925624\n",
      "Calinski-Harabasz Score‰∏∫Ôºö 1140.6267095034884\n",
      "Davies-Bouldin Score‰∏∫Ôºö 4.084639800130624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "# ‰ΩøÁî®KMeansÁÆóÊ≥ïÂØπÊñáÊú¨Êï∞ÊçÆËøõË°åËÅöÁ±ª\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(ndata)\n",
    "\n",
    "# ËæìÂá∫ËÅöÁ±ªÁªìÊûú\n",
    "clusters = kmeans.predict(ndata)\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# ËÆ°ÁÆóËÅöÁ±ªÊïàÊûúÁöÑËΩÆÂªìÁ≥ªÊï∞\n",
    "score = silhouette_score(ndata, clusters)\n",
    "print(\"ËΩÆÂªìÁ≥ªÊï∞‰∏∫Ôºö\", score)\n",
    "# ËÆ°ÁÆóCalinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(ndata, clusters)\n",
    "print(\"Calinski-Harabasz Score‰∏∫Ôºö\", score_ch)\n",
    "# ËÆ°ÁÆóDavies-Bouldin Score\n",
    "score_db = davies_bouldin_score(ndata, clusters)\n",
    "print(\"Davies-Bouldin Score‰∏∫Ôºö\", score_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      2\u001b[0m embedder \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m data_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      5\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(data_embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:434\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    425\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m         output_attentions,\n\u001b[0;32m    433\u001b[0m     )\n\u001b[1;32m--> 434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:384\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 384\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    386\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "data_embeddings = embedder.encode(data)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(data_embeddings)\n",
    "clusters = kmeans.predict(data_embeddings)\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# ËÆ°ÁÆóËÅöÁ±ªÊïàÊûúÁöÑËΩÆÂªìÁ≥ªÊï∞\n",
    "score = silhouette_score(data_embeddings, clusters)\n",
    "print(\"ËΩÆÂªìÁ≥ªÊï∞‰∏∫Ôºö\", score)\n",
    "# ËÆ°ÁÆóCalinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(data_embeddings, clusters)\n",
    "print(\"Calinski-Harabasz Score‰∏∫Ôºö\", score_ch)\n",
    "# ËÆ°ÁÆóDavies-Bouldin Score\n",
    "score_db = davies_bouldin_score(data_embeddings, clusters)\n",
    "print(\"Davies-Bouldin Score‰∏∫Ôºö\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# ‰ΩøÁî®DBSCANÁÆóÊ≥ïÂØπÊñáÊú¨Êï∞ÊçÆËøõË°åËÅöÁ±ª\n",
    "dbscan_cluster = DBSCAN(eps=0.5, min_samples=2)\n",
    "dbscan_cluster.fit(data_embeddings)\n",
    "\n",
    "# ËæìÂá∫ËÅöÁ±ªÁªìÊûú\n",
    "clusters = dbscan_cluster.labels_\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters[:10])\n",
    "#02\n",
    "# ËÆ°ÁÆóËÅöÁ±ªÊïàÊûúÁöÑËΩÆÂªìÁ≥ªÊï∞\n",
    "score = silhouette_score(data_embeddings, clusters)\n",
    "print(\"ËΩÆÂªìÁ≥ªÊï∞‰∏∫Ôºö\", score)\n",
    "# ËÆ°ÁÆóCalinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(data_embeddings, clusters)\n",
    "print(\"Calinski-Harabasz Score‰∏∫Ôºö\", score_ch)\n",
    "# ËÆ°ÁÆóDavies-Bouldin Score\n",
    "score_db = davies_bouldin_score(data_embeddings, clusters)\n",
    "print(\"Davies-Bouldin Score‰∏∫Ôºö\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# ‰ΩøÁî®DBSCANÁÆóÊ≥ïÂØπÊñáÊú¨Êï∞ÊçÆËøõË°åËÅöÁ±ª\n",
    "dbscan_cluster = DBSCAN(eps=0.5, min_samples=2)\n",
    "dbscan_cluster.fit(ndata)\n",
    "\n",
    "# ËæìÂá∫ËÅöÁ±ªÁªìÊûú\n",
    "clusters = dbscan_cluster.labels_\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters[:10])\n",
    "#02\n",
    "# ËÆ°ÁÆóËÅöÁ±ªÊïàÊûúÁöÑËΩÆÂªìÁ≥ªÊï∞\n",
    "score = silhouette_score(ndata, clusters)\n",
    "print(\"ËΩÆÂªìÁ≥ªÊï∞‰∏∫Ôºö\", score)\n",
    "# ËÆ°ÁÆóCalinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(ndata, clusters)\n",
    "print(\"Calinski-Harabasz Score‰∏∫Ôºö\", score_ch)\n",
    "# ËÆ°ÁÆóDavies-Bouldin Score\n",
    "score_db = davies_bouldin_score(ndata, clusters)\n",
    "print(\"Davies-Bouldin Score‰∏∫Ôºö\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "# ‰ΩøÁî®È´òÊñØÊ∑∑ÂêàÊ®°ÂûãËÅöÁ±ªÁÆóÊ≥ïÂØπÊñáÊú¨Êï∞ÊçÆËøõË°åËÅöÁ±ª\n",
    "gmm_cluster = GaussianMixture(n_components=5, random_state=42)\n",
    "gmm_cluster.fit(ndata)\n",
    "\n",
    "# ËæìÂá∫ËÅöÁ±ªÁªìÊûú\n",
    "clusters = gmm_cluster.predict(ndata)\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# ËÆ°ÁÆóËÅöÁ±ªÊïàÊûúÁöÑËΩÆÂªìÁ≥ªÊï∞\n",
    "score = silhouette_score(ndata, clusters)\n",
    "print(\"ËΩÆÂªìÁ≥ªÊï∞‰∏∫Ôºö\", score)\n",
    "# ËÆ°ÁÆóCalinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(ndata, clusters)\n",
    "print(\"Calinski-Harabasz Score‰∏∫Ôºö\", score_ch)\n",
    "# ËÆ°ÁÆóDavies-Bouldin Score\n",
    "score_db = davies_bouldin_score(ndata, clusters)\n",
    "print(\"Davies-Bouldin Score‰∏∫Ôºö\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "# ‰ΩøÁî®È´òÊñØÊ∑∑ÂêàÊ®°ÂûãËÅöÁ±ªÁÆóÊ≥ïÂØπÊñáÊú¨Êï∞ÊçÆËøõË°åËÅöÁ±ª\n",
    "gmm_cluster = GaussianMixture(n_components=5, random_state=42)\n",
    "gmm_cluster.fit(data_embeddings)\n",
    "\n",
    "# ËæìÂá∫ËÅöÁ±ªÁªìÊûú\n",
    "clusters = gmm_cluster.predict(data_embeddings)\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# ËÆ°ÁÆóËÅöÁ±ªÊïàÊûúÁöÑËΩÆÂªìÁ≥ªÊï∞\n",
    "score = silhouette_score(data_embeddings, clusters)\n",
    "print(\"ËΩÆÂªìÁ≥ªÊï∞‰∏∫Ôºö\", score)\n",
    "# ËÆ°ÁÆóCalinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(data_embeddings, clusters)\n",
    "print(\"Calinski-Harabasz Score‰∏∫Ôºö\", score_ch)\n",
    "# ËÆ°ÁÆóDavies-Bouldin Score\n",
    "score_db = davies_bouldin_score(data_embeddings, clusters)\n",
    "print(\"Davies-Bouldin Score‰∏∫Ôºö\", score_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ÂàùÂßãÂåñHDBSCANÊ®°Âûã\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric='euclidean')\n",
    "\n",
    "# ËÆ≠ÁªÉÊ®°Âûã\n",
    "clusterer.fit(ndata)\n",
    "# ËæìÂá∫ËÅöÁ±ªÁªìÊûú\n",
    "clusters = clusterer.predict(ndata)\n",
    "\n",
    "data_clusters = pd.DataFrame({'text': data, 'cluster': clusters})\n",
    "print(data_clusters.head())\n",
    "# ËÆ°ÁÆóËÅöÁ±ªÊïàÊûúÁöÑËΩÆÂªìÁ≥ªÊï∞\n",
    "score = silhouette_score(ndata, clusters)\n",
    "print(\"ËΩÆÂªìÁ≥ªÊï∞‰∏∫Ôºö\", score)\n",
    "# ËÆ°ÁÆóCalinski-Harabasz Score\n",
    "score_ch = calinski_harabasz_score(ndata, clusters)\n",
    "print(\"Calinski-Harabasz Score‰∏∫Ôºö\", score_ch)\n",
    "# ËÆ°ÁÆóDavies-Bouldin Score\n",
    "score_db = davies_bouldin_score(ndata, clusters)\n",
    "print(\"Davies-Bouldin Score‰∏∫Ôºö\", score_db)\n",
    "# ÂèØËßÜÂåñËÅöÁ±ªÁªìÊûú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced = TruncatedSVD(n_components=50, random_state=0).fit_transform(ndata)\n",
    "X_embedded = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(X_reduced)\n",
    "plt.scatter(X_embedded[:,0],X_embedded[:,1], c=clusterer.labels_, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "#\n",
    "# from sklearn.feature_extraction import text\n",
    "# from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "# my_stop_words = text.ENGLISH_STOP_WORDS.union([\"portuguese\"])\n",
    "# # ÂàõÂª∫SnowballËØçÂπ≤ÊèêÂèñÂô®\n",
    "# stemmer = SnowballStemmer(\"portuguese\")\n",
    "#\n",
    "# # ÂàõÂª∫TF-IDFÂêëÈáèÂåñÂô®ÔºåÂπ∂ÊåáÂÆö‰ΩøÁî®ËØçÂπ≤ÊèêÂèñÂô®\n",
    "# vectorizer = TfidfVectorizer(stop_words=my_stop_words, tokenizer=lambda x: [stemmer.stem(word) for word in x.split()])\n",
    "#\n",
    "# # ÂØπÊñáÊú¨Êï∞ÊçÆËøõË°åÂêëÈáèÂåñ\n",
    "# tfidf = vectorizer.fit_transform(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
