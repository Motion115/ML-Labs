{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\Python\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index=302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 加载数据，并转换为torch.Tensor\n",
    "# trainData = pd.read_pickle('../processed/corax_train.pkl')\n",
    "# testData = pd.read_pickle('../processed/corax_test.pkl')\n",
    "# validationData=pd.read_pickle('../processed/corax_validation.pkl')\n",
    "# allData=pd.concat([trainData,testData,validationData],axis=0)\n",
    "\n",
    "# # trainLabel = trainData[label_index]\n",
    "# # trainData = trainData.drop(columns=label_index)\n",
    "\n",
    "# # testLabel = testData[label_index]\n",
    "# # testData = testData.drop(columns=label_index)\n",
    "\n",
    "# # validationLabel = validationData[label_index]\n",
    "# # validationData = validationData.drop(columns=label_index)\n",
    "\n",
    "# allLabel= allData[label_index]\n",
    "# allData = allData.drop(columns=label_index)\n",
    "\n",
    "# G=nx.read_gpickle('../processed/corax_graph.gpickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.read_pickle('../corpus/corax_features.pkl')\n",
    "label = pd.read_pickle('../corpus/corax_labels.pkl')\n",
    "adj_matrix=pd.read_pickle('../corpus/corax_adj.pkl')\n",
    "\n",
    "label_onehot = torch.tensor(label)\n",
    "y = torch.topk(label_onehot, 1)[1].squeeze(1).numpy()\n",
    "\n",
    "G = nx.from_numpy_matrix(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allLabel.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2680, 302], edge_index=[2, 5148], y=[2680], train_mask=[2680], val_mask=[2680], test_mask=[2680])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# 确定x\n",
    "x = torch.tensor(feature, dtype=torch.float)\n",
    "\n",
    "# 确定edge_index\n",
    "edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "\n",
    "# 确定y\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "train_mask = np.zeros(x.shape[0], dtype=bool)\n",
    "val_mask = np.zeros(x.shape[0], dtype=bool)\n",
    "test_mask = np.zeros(x.shape[0], dtype=bool)\n",
    "\n",
    "train_mask[0:1180]=True\n",
    "val_mask[1180:1680]=True\n",
    "test_mask[1680:2681]=True\n",
    "\n",
    "data=Data(x=x,edge_index=edge_index,y=y,train_mask=train_mask,val_mask=val_mask,test_mask=test_mask)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,GATConv,SAGEConv\n",
    "\n",
    "\n",
    "# GCN：acc 0.7810\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT：acc 0.8060\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels=num_node_features,\n",
    "                                    out_channels=16,\n",
    "                                    heads=2)\n",
    "        self.conv2 = GATConv(in_channels=2*16,\n",
    "                                    out_channels=num_classes,\n",
    "                                    heads=1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.78左右\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_node_features, 16)\n",
    "        self.conv2 = SAGEConv(16, num_classes)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features=data.x.shape[1]\n",
    "num_classes = 7\n",
    "model=GraphSAGE(num_node_features,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 loss 1.9664\n",
      "Epoch 001 loss 1.8723\n",
      "Epoch 002 loss 1.8240\n",
      "Epoch 003 loss 1.7750\n",
      "Epoch 004 loss 1.7217\n",
      "Epoch 005 loss 1.6760\n",
      "Epoch 006 loss 1.6188\n",
      "Epoch 007 loss 1.5724\n",
      "Epoch 008 loss 1.5316\n",
      "Epoch 009 loss 1.5162\n",
      "Epoch 010 loss 1.4582\n",
      "Epoch 011 loss 1.4319\n",
      "Epoch 012 loss 1.4106\n",
      "Epoch 013 loss 1.3676\n",
      "Epoch 014 loss 1.3452\n",
      "Epoch 015 loss 1.3111\n",
      "Epoch 016 loss 1.2998\n",
      "Epoch 017 loss 1.2745\n",
      "Epoch 018 loss 1.2437\n",
      "Epoch 019 loss 1.2606\n",
      "Epoch 020 loss 1.2205\n",
      "Epoch 021 loss 1.1848\n",
      "Epoch 022 loss 1.2036\n",
      "Epoch 023 loss 1.1333\n",
      "Epoch 024 loss 1.1576\n",
      "Epoch 025 loss 1.1256\n",
      "Epoch 026 loss 1.1215\n",
      "Epoch 027 loss 1.1130\n",
      "Epoch 028 loss 1.0953\n",
      "Epoch 029 loss 1.0684\n",
      "Epoch 030 loss 1.0689\n",
      "Epoch 031 loss 1.0576\n",
      "Epoch 032 loss 1.0057\n",
      "Epoch 033 loss 1.0175\n",
      "Epoch 034 loss 0.9882\n",
      "Epoch 035 loss 1.0010\n",
      "Epoch 036 loss 0.9535\n",
      "Epoch 037 loss 0.9673\n",
      "Epoch 038 loss 0.9739\n",
      "Epoch 039 loss 0.9429\n",
      "Epoch 040 loss 0.9148\n",
      "Epoch 041 loss 0.9406\n",
      "Epoch 042 loss 0.8907\n",
      "Epoch 043 loss 0.9036\n",
      "Epoch 044 loss 0.8719\n",
      "Epoch 045 loss 0.8505\n",
      "Epoch 046 loss 0.8638\n",
      "Epoch 047 loss 0.8756\n",
      "Epoch 048 loss 0.8708\n",
      "Epoch 049 loss 0.8552\n",
      "Epoch 050 loss 0.8612\n",
      "Epoch 051 loss 0.8446\n",
      "Epoch 052 loss 0.8059\n",
      "Epoch 053 loss 0.8495\n",
      "Epoch 054 loss 0.8166\n",
      "Epoch 055 loss 0.8254\n",
      "Epoch 056 loss 0.8121\n",
      "Epoch 057 loss 0.7937\n",
      "Epoch 058 loss 0.7858\n",
      "Epoch 059 loss 0.7529\n",
      "Epoch 060 loss 0.7695\n",
      "Epoch 061 loss 0.7569\n",
      "Epoch 062 loss 0.7451\n",
      "Epoch 063 loss 0.7762\n",
      "Epoch 064 loss 0.7329\n",
      "Epoch 065 loss 0.7324\n",
      "Epoch 066 loss 0.7039\n",
      "Epoch 067 loss 0.7204\n",
      "Epoch 068 loss 0.7577\n",
      "Epoch 069 loss 0.7163\n",
      "Epoch 070 loss 0.7370\n",
      "Epoch 071 loss 0.6996\n",
      "Epoch 072 loss 0.7129\n",
      "Epoch 073 loss 0.6815\n",
      "Epoch 074 loss 0.6960\n",
      "Epoch 075 loss 0.7030\n",
      "Epoch 076 loss 0.6791\n",
      "Epoch 077 loss 0.6885\n",
      "Epoch 078 loss 0.6661\n",
      "Epoch 079 loss 0.6749\n",
      "Epoch 080 loss 0.6445\n",
      "Epoch 081 loss 0.6822\n",
      "Epoch 082 loss 0.6549\n",
      "Epoch 083 loss 0.6657\n",
      "Epoch 084 loss 0.6420\n",
      "Epoch 085 loss 0.6804\n",
      "Epoch 086 loss 0.6514\n",
      "Epoch 087 loss 0.6155\n",
      "Epoch 088 loss 0.6307\n",
      "Epoch 089 loss 0.6543\n",
      "Epoch 090 loss 0.6493\n",
      "Epoch 091 loss 0.6125\n",
      "Epoch 092 loss 0.6302\n",
      "Epoch 093 loss 0.6355\n",
      "Epoch 094 loss 0.6125\n",
      "Epoch 095 loss 0.5998\n",
      "Epoch 096 loss 0.6066\n",
      "Epoch 097 loss 0.5885\n",
      "Epoch 098 loss 0.5944\n",
      "Epoch 099 loss 0.5910\n",
      "Epoch 100 loss 0.5912\n",
      "Epoch 101 loss 0.5892\n",
      "Epoch 102 loss 0.6181\n",
      "Epoch 103 loss 0.6003\n",
      "Epoch 104 loss 0.6005\n",
      "Epoch 105 loss 0.6057\n",
      "Epoch 106 loss 0.6069\n",
      "Epoch 107 loss 0.5721\n",
      "Epoch 108 loss 0.5550\n",
      "Epoch 109 loss 0.5880\n",
      "Epoch 110 loss 0.5729\n",
      "Epoch 111 loss 0.5868\n",
      "Epoch 112 loss 0.5939\n",
      "Epoch 113 loss 0.5805\n",
      "Epoch 114 loss 0.5587\n",
      "Epoch 115 loss 0.5824\n",
      "Epoch 116 loss 0.5412\n",
      "Epoch 117 loss 0.5532\n",
      "Epoch 118 loss 0.5823\n",
      "Epoch 119 loss 0.5298\n",
      "Epoch 120 loss 0.5914\n",
      "Epoch 121 loss 0.5508\n",
      "Epoch 122 loss 0.5511\n",
      "Epoch 123 loss 0.5801\n",
      "Epoch 124 loss 0.5511\n",
      "Epoch 125 loss 0.5486\n",
      "Epoch 126 loss 0.5337\n",
      "Epoch 127 loss 0.5314\n",
      "Epoch 128 loss 0.5538\n",
      "Epoch 129 loss 0.5355\n",
      "Epoch 130 loss 0.5214\n",
      "Epoch 131 loss 0.5540\n",
      "Epoch 132 loss 0.5214\n",
      "Epoch 133 loss 0.5275\n",
      "Epoch 134 loss 0.5491\n",
      "Epoch 135 loss 0.5266\n",
      "Epoch 136 loss 0.5111\n",
      "Epoch 137 loss 0.5366\n",
      "Epoch 138 loss 0.5216\n",
      "Epoch 139 loss 0.4923\n",
      "Epoch 140 loss 0.5162\n",
      "Epoch 141 loss 0.5264\n",
      "Epoch 142 loss 0.5332\n",
      "Epoch 143 loss 0.4797\n",
      "Epoch 144 loss 0.5368\n",
      "Epoch 145 loss 0.5122\n",
      "Epoch 146 loss 0.4989\n",
      "Epoch 147 loss 0.5029\n",
      "Epoch 148 loss 0.5185\n",
      "Epoch 149 loss 0.4857\n",
      "Epoch 150 loss 0.4957\n",
      "Epoch 151 loss 0.4889\n",
      "Epoch 152 loss 0.4914\n",
      "Epoch 153 loss 0.5228\n",
      "Epoch 154 loss 0.4644\n",
      "Epoch 155 loss 0.4903\n",
      "Epoch 156 loss 0.4926\n",
      "Epoch 157 loss 0.4974\n",
      "Epoch 158 loss 0.4830\n",
      "Epoch 159 loss 0.4813\n",
      "Epoch 160 loss 0.5026\n",
      "Epoch 161 loss 0.4823\n",
      "Epoch 162 loss 0.5032\n",
      "Epoch 163 loss 0.4778\n",
      "Epoch 164 loss 0.4739\n",
      "Epoch 165 loss 0.4384\n",
      "Epoch 166 loss 0.4949\n",
      "Epoch 167 loss 0.4856\n",
      "Epoch 168 loss 0.4756\n",
      "Epoch 169 loss 0.4485\n",
      "Epoch 170 loss 0.4916\n",
      "Epoch 171 loss 0.4549\n",
      "Epoch 172 loss 0.4919\n",
      "Epoch 173 loss 0.4831\n",
      "Epoch 174 loss 0.4871\n",
      "Epoch 175 loss 0.4737\n",
      "Epoch 176 loss 0.5013\n",
      "Epoch 177 loss 0.4538\n",
      "Epoch 178 loss 0.4538\n",
      "Epoch 179 loss 0.4552\n",
      "Epoch 180 loss 0.4622\n",
      "Epoch 181 loss 0.4662\n",
      "Epoch 182 loss 0.4660\n",
      "Epoch 183 loss 0.4784\n",
      "Epoch 184 loss 0.4343\n",
      "Epoch 185 loss 0.4544\n",
      "Epoch 186 loss 0.4458\n",
      "Epoch 187 loss 0.4375\n",
      "Epoch 188 loss 0.4514\n",
      "Epoch 189 loss 0.4737\n",
      "Epoch 190 loss 0.4466\n",
      "Epoch 191 loss 0.4909\n",
      "Epoch 192 loss 0.4324\n",
      "Epoch 193 loss 0.4539\n",
      "Epoch 194 loss 0.4378\n",
      "Epoch 195 loss 0.4115\n",
      "Epoch 196 loss 0.4137\n",
      "Epoch 197 loss 0.4375\n",
      "Epoch 198 loss 0.4634\n",
      "Epoch 199 loss 0.4462\n",
      "GCN Accuracy: 0.7800\n"
      "Epoch 000 loss 1.9464\n",
      "Epoch 001 loss 1.9355\n",
      "Epoch 002 loss 1.9186\n",
      "Epoch 003 loss 1.9121\n",
      "Epoch 004 loss 1.8936\n",
      "Epoch 005 loss 1.8828\n",
      "Epoch 006 loss 1.8909\n",
      "Epoch 007 loss 1.8458\n",
      "Epoch 008 loss 1.8555\n",
      "Epoch 009 loss 1.8487\n",
      "Epoch 010 loss 1.8612\n",
      "Epoch 011 loss 1.8302\n",
      "Epoch 012 loss 1.8214\n",
      "Epoch 013 loss 1.8267\n",
      "Epoch 014 loss 1.8027\n",
      "Epoch 015 loss 1.8026\n",
      "Epoch 016 loss 1.8257\n",
      "Epoch 017 loss 1.8072\n",
      "Epoch 018 loss 1.7912\n",
      "Epoch 019 loss 1.8161\n",
      "Epoch 020 loss 1.7732\n",
      "Epoch 021 loss 1.8079\n",
      "Epoch 022 loss 1.7949\n",
      "Epoch 023 loss 1.7833\n",
      "Epoch 024 loss 1.7711\n",
      "Epoch 025 loss 1.7744\n",
      "Epoch 026 loss 1.7852\n",
      "Epoch 027 loss 1.7877\n",
      "Epoch 028 loss 1.7716\n",
      "Epoch 029 loss 1.7795\n",
      "Epoch 030 loss 1.7684\n",
      "Epoch 031 loss 1.7498\n",
      "Epoch 032 loss 1.7818\n",
      "Epoch 033 loss 1.7653\n",
      "Epoch 034 loss 1.7684\n",
      "Epoch 035 loss 1.7503\n",
      "Epoch 036 loss 1.7552\n",
      "Epoch 037 loss 1.7658\n",
      "Epoch 038 loss 1.7616\n",
      "Epoch 039 loss 1.7814\n",
      "Epoch 040 loss 1.7704\n",
      "Epoch 041 loss 1.7463\n",
      "Epoch 042 loss 1.7625\n",
      "Epoch 043 loss 1.7455\n",
      "Epoch 044 loss 1.7360\n",
      "Epoch 045 loss 1.7385\n",
      "Epoch 046 loss 1.7537\n",
      "Epoch 047 loss 1.7432\n",
      "Epoch 048 loss 1.7445\n",
      "Epoch 049 loss 1.7578\n",
      "Epoch 050 loss 1.7546\n",
      "Epoch 051 loss 1.7440\n",
      "Epoch 052 loss 1.7514\n",
      "Epoch 053 loss 1.7587\n",
      "Epoch 054 loss 1.7443\n",
      "Epoch 055 loss 1.7251\n",
      "Epoch 056 loss 1.7254\n",
      "Epoch 057 loss 1.7459\n",
      "Epoch 058 loss 1.7271\n",
      "Epoch 059 loss 1.7418\n",
      "Epoch 060 loss 1.7371\n",
      "Epoch 061 loss 1.7539\n",
      "Epoch 062 loss 1.7294\n",
      "Epoch 063 loss 1.7263\n",
      "Epoch 064 loss 1.7402\n",
      "Epoch 065 loss 1.7367\n",
      "Epoch 066 loss 1.7244\n",
      "Epoch 067 loss 1.7344\n",
      "Epoch 068 loss 1.7208\n",
      "Epoch 069 loss 1.7257\n",
      "Epoch 070 loss 1.7213\n",
      "Epoch 071 loss 1.7234\n",
      "Epoch 072 loss 1.7444\n",
      "Epoch 073 loss 1.7277\n",
      "Epoch 074 loss 1.7178\n",
      "Epoch 075 loss 1.7209\n",
      "Epoch 076 loss 1.7025\n",
      "Epoch 077 loss 1.7269\n",
      "Epoch 078 loss 1.7305\n",
      "Epoch 079 loss 1.7283\n",
      "Epoch 080 loss 1.7182\n",
      "Epoch 081 loss 1.7301\n",
      "Epoch 082 loss 1.7152\n",
      "Epoch 083 loss 1.7212\n",
      "Epoch 084 loss 1.7278\n",
      "Epoch 085 loss 1.7188\n",
      "Epoch 086 loss 1.7251\n",
      "Epoch 087 loss 1.7155\n",
      "Epoch 088 loss 1.7380\n",
      "Epoch 089 loss 1.7221\n",
      "Epoch 090 loss 1.7242\n",
      "Epoch 091 loss 1.7256\n",
      "Epoch 092 loss 1.7044\n",
      "Epoch 093 loss 1.7250\n",
      "Epoch 094 loss 1.7215\n",
      "Epoch 095 loss 1.7099\n",
      "Epoch 096 loss 1.7128\n",
      "Epoch 097 loss 1.7268\n",
      "Epoch 098 loss 1.7042\n",
      "Epoch 099 loss 1.7298\n",
      "Epoch 100 loss 1.7173\n",
      "Epoch 101 loss 1.7071\n",
      "Epoch 102 loss 1.7094\n",
      "Epoch 103 loss 1.7045\n",
      "Epoch 104 loss 1.7214\n",
      "Epoch 105 loss 1.7220\n",
      "Epoch 106 loss 1.7147\n",
      "Epoch 107 loss 1.7125\n",
      "Epoch 108 loss 1.7041\n",
      "Epoch 109 loss 1.7322\n",
      "Epoch 110 loss 1.7048\n",
      "Epoch 111 loss 1.7081\n",
      "Epoch 112 loss 1.7200\n",
      "Epoch 113 loss 1.7411\n",
      "Epoch 114 loss 1.7039\n",
      "Epoch 115 loss 1.7046\n",
      "Epoch 116 loss 1.7086\n",
      "Epoch 117 loss 1.7014\n",
      "Epoch 118 loss 1.7105\n",
      "Epoch 119 loss 1.6914\n",
      "Epoch 120 loss 1.7083\n",
      "Epoch 121 loss 1.7161\n",
      "Epoch 122 loss 1.7048\n",
      "Epoch 123 loss 1.7106\n",
      "Epoch 124 loss 1.7025\n",
      "Epoch 125 loss 1.7145\n",
      "Epoch 126 loss 1.7023\n",
      "Epoch 127 loss 1.7062\n",
      "Epoch 128 loss 1.6962\n",
      "Epoch 129 loss 1.7037\n",
      "Epoch 130 loss 1.7039\n",
      "Epoch 131 loss 1.7111\n",
      "Epoch 132 loss 1.7045\n",
      "Epoch 133 loss 1.6975\n",
      "Epoch 134 loss 1.6987\n",
      "Epoch 135 loss 1.6968\n",
      "Epoch 136 loss 1.7050\n",
      "Epoch 137 loss 1.7027\n",
      "Epoch 138 loss 1.7100\n",
      "Epoch 139 loss 1.6876\n",
      "Epoch 140 loss 1.6961\n",
      "Epoch 141 loss 1.6991\n",
      "Epoch 142 loss 1.7060\n",
      "Epoch 143 loss 1.6936\n",
      "Epoch 144 loss 1.7054\n",
      "Epoch 145 loss 1.6916\n",
      "Epoch 146 loss 1.6943\n",
      "Epoch 147 loss 1.6894\n",
      "Epoch 148 loss 1.6979\n",
      "Epoch 149 loss 1.6850\n",
      "Epoch 150 loss 1.7056\n",
      "Epoch 151 loss 1.7066\n",
      "Epoch 152 loss 1.6856\n",
      "Epoch 153 loss 1.6869\n",
      "Epoch 154 loss 1.6920\n",
      "Epoch 155 loss 1.7098\n",
      "Epoch 156 loss 1.6867\n",
      "Epoch 157 loss 1.7021\n",
      "Epoch 158 loss 1.6872\n",
      "Epoch 159 loss 1.6898\n",
      "Epoch 160 loss 1.7158\n",
      "Epoch 161 loss 1.7023\n",
      "Epoch 162 loss 1.6948\n",
      "Epoch 163 loss 1.6992\n",
      "Epoch 164 loss 1.7026\n",
      "Epoch 165 loss 1.6833\n",
      "Epoch 166 loss 1.6797\n",
      "Epoch 167 loss 1.6957\n",
      "Epoch 168 loss 1.6892\n",
      "Epoch 169 loss 1.6850\n",
      "Epoch 170 loss 1.7055\n",
      "Epoch 171 loss 1.6978\n",
      "Epoch 172 loss 1.7038\n",
      "Epoch 173 loss 1.6875\n",
      "Epoch 174 loss 1.6967\n",
      "Epoch 175 loss 1.6932\n",
      "Epoch 176 loss 1.7057\n",
      "Epoch 177 loss 1.7033\n",
      "Epoch 178 loss 1.6885\n",
      "Epoch 179 loss 1.6901\n",
      "Epoch 180 loss 1.6750\n",
      "Epoch 181 loss 1.6895\n",
      "Epoch 182 loss 1.7041\n",
      "Epoch 183 loss 1.6940\n",
      "Epoch 184 loss 1.7147\n",
      "Epoch 185 loss 1.6921\n",
      "Epoch 186 loss 1.6970\n",
      "Epoch 187 loss 1.6987\n",
      "Epoch 188 loss 1.6860\n",
      "Epoch 189 loss 1.6737\n",
      "Epoch 190 loss 1.6974\n",
      "Epoch 191 loss 1.6809\n",
      "Epoch 192 loss 1.6807\n",
      "Epoch 193 loss 1.6797\n",
      "Epoch 194 loss 1.6822\n",
      "Epoch 195 loss 1.6778\n",
      "Epoch 196 loss 1.6783\n",
      "Epoch 197 loss 1.6991\n",
      "Epoch 198 loss 1.6782\n",
      "Epoch 199 loss 1.7070\n",
      "GCN Accuracy: 0.7220\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    out = model(data)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:03d} loss {:.4f}'.format(epoch, loss.item()))\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print('GCN Accuracy: {:.4f}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
