{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import CoraGraphDataset\n",
    "\n",
    "dataset = CoraGraphDataset()\n",
    "graph = dataset[0]\n",
    "nlabels = graph.ndata['label']\n",
    "num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnnlens import Writer\n",
    "\n",
    "# Specify the path to create a new directory for dumping data files.\n",
    "writer = Writer('tutorial_nlabel')\n",
    "writer.add_graph(name='Cora', graph=graph, \n",
    "                 nlabels=nlabels, num_nlabel_types=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN with one layer...\n",
      "Training GCN with two layers...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "# Define a class for GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 num_classes,\n",
    "                 num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GraphConv(in_feats, num_classes))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GraphConv(num_classes, num_classes))\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "\n",
    "# Define a function to train a GCN with the specified number of layers \n",
    "# and return the predictions\n",
    "def train_gcn(g, num_layers, num_classes):\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    model = GCN(in_feats=features.shape[1],\n",
    "                num_classes=num_classes,\n",
    "                num_layers=num_layers)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "  \n",
    "    num_epochs = 200\n",
    "    model.train()\n",
    "    for _ in range(num_epochs):\n",
    "        logits = model(g, features)\n",
    "        loss = loss_func(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "    model.eval()\n",
    "    predictions = model(g, features)\n",
    "    _, predicted_classes = torch.max(predictions, dim=1)\n",
    "    return predicted_classes\n",
    "\n",
    "print(\"Training GCN with one layer...\")\n",
    "predictions_one_layer = train_gcn(graph, num_layers=1, num_classes=num_classes)\n",
    "print(\"Training GCN with two layers...\")\n",
    "predictions_two_layers = train_gcn(graph, num_layers=2, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gsage with one layer...\n",
      "tensor(0.7640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import SAGEConv\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.gsage_layers = nn.ModuleList()\n",
    "        self.gsage_layers.append(SAGEConv(num_node_features, 16, aggregator_type='mean'))\n",
    "        self.gsage_layers.append(SAGEConv(16, num_classes, aggregator_type='mean'))\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        for layer in self.gsage_layers:\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "\n",
    "def train_gsage(g, num_classes):\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    model = GraphSAGE(num_node_features=features.shape[1],\n",
    "                num_classes=num_classes)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "  \n",
    "    num_epochs = 200\n",
    "    model.train()\n",
    "    for _ in range(num_epochs):\n",
    "        logits = model(g, features)\n",
    "        loss = loss_func(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "    model.eval()\n",
    "    predictions = model(g, features)\n",
    "    _, predicted_classes = torch.max(predictions, dim=1)\n",
    "\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    test_acc = (predicted_classes[test_mask] == labels[test_mask]).float().mean()\n",
    "    print(test_acc)\n",
    "\n",
    "    return predicted_classes\n",
    "\n",
    "print(\"Training Gsage with one layer...\")\n",
    "predictions_gsage = train_gsage(graph, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAT with two layers...\n",
      "tensor(0.7560)\n",
      "Training GAT with three layers...\n",
      "tensor(0.6910)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GATConv\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 num_classes,\n",
    "                 heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        # input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(in_dim, num_hidden, heads[0]))\n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers - 1):\n",
    "            # due to multi-head, in_dim = num_hidden * number of heads in the previous layer\n",
    "            self.gat_layers.append(GATConv(num_hidden * heads[l-1], num_hidden, heads[l]))\n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(num_hidden * heads[-2], num_classes, heads[-1]))\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        attns = []\n",
    "        for l in range(self.num_layers - 1):\n",
    "            h, attn = self.gat_layers[l](g, h, get_attention=True)\n",
    "            h = h.flatten(1)\n",
    "            attns.append(attn)\n",
    "        # output projection\n",
    "        logits, attn = self.gat_layers[-1](g, h, get_attention=True)\n",
    "        logits = logits.mean(1)\n",
    "        attns.append(attn)\n",
    "        return logits, attns\n",
    "\n",
    "def convert_attns_to_dict(attns):\n",
    "    attn_dict = {}\n",
    "    for layer, attn_list in enumerate(attns):\n",
    "        attn_list = attn_list.squeeze(2).transpose(0, 1)\n",
    "        for head, attn in enumerate(attn_list):\n",
    "            head_name = \"L{}_H{}\".format(layer, head)\n",
    "            attn_dict[head_name] = attn\n",
    "    return attn_dict\n",
    "\n",
    "def train_gat(g, num_layers, heads, num_classes):\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    model = GAT(num_layers=num_layers,\n",
    "                in_dim=features.shape[1],\n",
    "                num_hidden=8,\n",
    "                num_classes=num_classes,\n",
    "                heads=heads)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    \n",
    "    num_epochs = 35\n",
    "    model.train()\n",
    "    for epochs in range(num_epochs):\n",
    "        logits, _ = model(g, features)\n",
    "        loss = loss_func(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    predictions, attns = model(g, features)\n",
    "    _, predicted_classes = torch.max(predictions, dim=1)\n",
    "    attn_dict = convert_attns_to_dict(attns)\n",
    "    \n",
    "    test_mask = g.ndata['test_mask']\n",
    "    test_acc = (predicted_classes[test_mask] == labels[test_mask]).float().mean()\n",
    "    print(test_acc)\n",
    "\n",
    "    return predicted_classes, attn_dict\n",
    "\n",
    "\n",
    "print(\"Training GAT with two layers...\")\n",
    "predictions_gat_two_layers, attn_dict_two_layers = train_gat(\n",
    "    graph, num_layers=2, heads=[2,1], num_classes=num_classes)\n",
    "\n",
    "print(\"Training GAT with three layers...\")\n",
    "predictions_gat_three_layers, attn_dict_three_layers = train_gat(\n",
    "    graph, num_layers=3, heads=[4,2,1], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the predictions to local files\n",
    "writer.add_model(graph_name='Cora', model_name='GCN_L1',\n",
    "                 nlabels=predictions_one_layer)\n",
    "\n",
    "writer.add_model(graph_name='Cora', model_name='GCN_L2',\n",
    "                 nlabels=predictions_two_layers)\n",
    "\n",
    "writer.add_model(graph_name='Cora', model_name='GraphSage',\n",
    "                 nlabels=predictions_gsage)\n",
    "\n",
    "writer.add_model(graph_name='Cora', model_name='GAT_L2', \n",
    "                 nlabels=predictions_gat_two_layers, eweights=attn_dict_two_layers)\n",
    "\n",
    "writer.add_model(graph_name='Cora', model_name='GAT_L3', \n",
    "                 nlabels=predictions_gat_three_layers, eweights=attn_dict_three_layers)\n",
    "\n",
    "# Finish dumping\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
