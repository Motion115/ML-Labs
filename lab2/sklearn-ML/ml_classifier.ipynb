{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "using traditional machine learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import load_cora\n",
    "X_train, y_train, X_test, y_test = load_cora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%f\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "def train_test(model_name, model, X, y, X_test, y_test):\n",
    "    # train the model\n",
    "    model.fit(X, y)\n",
    "    # calculate train set metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    confusion_matrix = confusion_matrix(y_test, y_pred, labels=[1,0])\n",
    "    classification_report = classification_report(y_test, y_pred)\n",
    "    accuracy_score = accuracy_score(y_test, y_pred)\n",
    "    print(f'【{model_name}】')\n",
    "    print(f'confusion_matrix:\\n{confusion_matrix}')\n",
    "    print(f'accuracy_score:\\n{accuracy_score}')\n",
    "    print(f'classification_report:\\n{classification_report}')\n",
    "    # sns.heatmap(confusion_matrix,cmap='Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【naive bayes-GaussianNB】\n",
      "confusion_matrix:\n",
      "[[55  5]\n",
      " [20 51]]\n",
      "accuracy_score:\n",
      "0.556\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.39      0.39       130\n",
      "           1       0.44      0.60      0.50        92\n",
      "           2       0.73      0.63      0.68       143\n",
      "           3       0.72      0.59      0.65       318\n",
      "           4       0.53      0.58      0.55       149\n",
      "           5       0.44      0.63      0.52       105\n",
      "           6       0.44      0.32      0.37        63\n",
      "\n",
      "    accuracy                           0.56      1000\n",
      "   macro avg       0.53      0.53      0.52      1000\n",
      "weighted avg       0.58      0.56      0.56      1000\n",
      "\n",
      "【naive bayes-BernoulliNB】\n",
      "confusion_matrix:\n",
      "[[48  6]\n",
      " [21 49]]\n",
      "accuracy_score:\n",
      "0.549\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       130\n",
      "           1       0.40      0.52      0.45        92\n",
      "           2       0.66      0.64      0.65       143\n",
      "           3       0.72      0.61      0.66       318\n",
      "           4       0.51      0.60      0.55       149\n",
      "           5       0.54      0.57      0.55       105\n",
      "           6       0.32      0.29      0.30        63\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.50      0.51      0.51      1000\n",
      "weighted avg       0.56      0.55      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "train_test(\"naive bayes-GaussianNB\",GaussianNB(), X_train, y_train, X_test, y_test)\n",
    "train_test(\"naive bayes-BernoulliNB\",BernoulliNB(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Decision Tree Classifier】\n",
      "confusion_matrix:\n",
      "[[26 11]\n",
      " [ 9 38]]\n",
      "accuracy_score:\n",
      "0.375\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.29      0.28       130\n",
      "           1       0.29      0.28      0.28        92\n",
      "           2       0.41      0.40      0.41       143\n",
      "           3       0.49      0.45      0.47       318\n",
      "           4       0.38      0.40      0.39       149\n",
      "           5       0.35      0.38      0.37       105\n",
      "           6       0.18      0.19      0.18        63\n",
      "\n",
      "    accuracy                           0.38      1000\n",
      "   macro avg       0.34      0.34      0.34      1000\n",
      "weighted avg       0.38      0.38      0.38      1000\n",
      "\n",
      "【Decision Tree Classifier】\n",
      "confusion_matrix:\n",
      "[[22 12]\n",
      " [16 32]]\n",
      "accuracy_score:\n",
      "0.346\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.25      0.22       130\n",
      "           1       0.26      0.24      0.25        92\n",
      "           2       0.44      0.38      0.41       143\n",
      "           3       0.48      0.47      0.48       318\n",
      "           4       0.32      0.32      0.32       149\n",
      "           5       0.28      0.28      0.28       105\n",
      "           6       0.15      0.17      0.16        63\n",
      "\n",
      "    accuracy                           0.35      1000\n",
      "   macro avg       0.31      0.30      0.30      1000\n",
      "weighted avg       0.35      0.35      0.35      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# CART基于基尼系数\n",
    "train_test(\"Decision Tree Classifier\",DecisionTreeClassifier(criterion='gini'), X_train, y_train, X_test, y_test)\n",
    "# ID3和C4.5基于信息熵\n",
    "train_test(\"Decision Tree Classifier\",DecisionTreeClassifier(criterion='entropy'), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【KNeighbors Classifier】\n",
      "confusion_matrix:\n",
      "[[47  4]\n",
      " [11 59]]\n",
      "accuracy_score:\n",
      "0.603\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.45      0.45       130\n",
      "           1       0.53      0.51      0.52        92\n",
      "           2       0.62      0.70      0.66       143\n",
      "           3       0.68      0.72      0.70       318\n",
      "           4       0.66      0.56      0.61       149\n",
      "           5       0.52      0.64      0.58       105\n",
      "           6       0.74      0.27      0.40        63\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.60      0.55      0.56      1000\n",
      "weighted avg       0.61      0.60      0.60      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "train_test(\"KNeighbors Classifier\",KNeighborsClassifier(n_neighbors=7), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【SVC-linear】\n",
      "confusion_matrix:\n",
      "[[48 10]\n",
      " [ 6 58]]\n",
      "accuracy_score:\n",
      "0.611\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.45      0.39       130\n",
      "           1       0.57      0.52      0.55        92\n",
      "           2       0.71      0.71      0.71       143\n",
      "           3       0.72      0.71      0.71       318\n",
      "           4       0.62      0.62      0.62       149\n",
      "           5       0.64      0.54      0.59       105\n",
      "           6       0.55      0.43      0.48        63\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.59      0.57      0.58      1000\n",
      "weighted avg       0.62      0.61      0.61      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "# 选取线性核函数linear\n",
    "train_test(\"SVC-linear\",SVC(kernel = 'linear'), X_train, y_train, X_test, y_test)\n",
    "# # 选取非线性核函数poly(多项式核函数)和 rbf(高斯核函数)\n",
    "# train_test(\"SVC-poly\",SVC(kernel = 'poly'), X_train, y_train, X_test, y_test)\n",
    "# train_test(\"SVC-rbf\",SVC(kernel = 'rbf'), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Logistic Regression】\n",
      "confusion_matrix:\n",
      "[[40  7]\n",
      " [ 8 55]]\n",
      "accuracy_score:\n",
      "0.623\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.42      0.41       130\n",
      "           1       0.63      0.43      0.52        92\n",
      "           2       0.71      0.72      0.71       143\n",
      "           3       0.68      0.76      0.72       318\n",
      "           4       0.65      0.64      0.65       149\n",
      "           5       0.65      0.60      0.62       105\n",
      "           6       0.49      0.40      0.44        63\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.60      0.57      0.58      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "【Logistic Regression】\n",
      "confusion_matrix:\n",
      "[[40  7]\n",
      " [ 8 55]]\n",
      "accuracy_score:\n",
      "0.623\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.42      0.41       130\n",
      "           1       0.63      0.43      0.52        92\n",
      "           2       0.71      0.72      0.71       143\n",
      "           3       0.68      0.76      0.72       318\n",
      "           4       0.65      0.64      0.65       149\n",
      "           5       0.65      0.60      0.62       105\n",
      "           6       0.49      0.40      0.44        63\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.60      0.57      0.58      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "【Logistic Regression】\n",
      "confusion_matrix:\n",
      "[[36  9]\n",
      " [ 6 55]]\n",
      "accuracy_score:\n",
      "0.629\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42       130\n",
      "           1       0.63      0.39      0.48        92\n",
      "           2       0.66      0.71      0.69       143\n",
      "           3       0.68      0.78      0.73       318\n",
      "           4       0.67      0.67      0.67       149\n",
      "           5       0.65      0.62      0.63       105\n",
      "           6       0.53      0.38      0.44        63\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.61      0.57      0.58      1000\n",
      "weighted avg       0.63      0.63      0.62      1000\n",
      "\n",
      "【Logistic Regression】\n",
      "confusion_matrix:\n",
      "[[40  7]\n",
      " [ 8 55]]\n",
      "accuracy_score:\n",
      "0.623\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.42      0.41       130\n",
      "           1       0.63      0.43      0.52        92\n",
      "           2       0.71      0.72      0.71       143\n",
      "           3       0.68      0.76      0.72       318\n",
      "           4       0.65      0.64      0.65       149\n",
      "           5       0.65      0.60      0.62       105\n",
      "           6       0.49      0.40      0.44        63\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.60      0.57      0.58      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xy\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "train_test(\"Logistic Regression\",LogisticRegression(solver='newton-cg'), X_train, y_train, X_test, y_test)\n",
    "train_test(\"Logistic Regression\",LogisticRegression(solver='lbfgs'), X_train, y_train, X_test, y_test)\n",
    "train_test(\"Logistic Regression\",LogisticRegression(solver='liblinear'), X_train, y_train, X_test, y_test)\n",
    "train_test(\"Logistic Regression\",LogisticRegression(solver='sag'), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n应用最小二乘法进行一维线性回归\\n求导直接求得系数\\n不加正则化项： w = 1/(X.T@X)@X.T@y\\n加L2正则化项： w = 1/(X.T@X+alpha*E)@X.T@y\\n'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "应用最小二乘法进行一维线性回归\n",
    "求导直接求得系数\n",
    "不加正则化项： w = 1/(X.T@X)@X.T@y\n",
    "加L2正则化项： w = 1/(X.T@X+alpha*E)@X.T@y\n",
    "\"\"\"\n",
    "# from\n",
    "# def sigmoid(z):\n",
    "#     s = 1/(1+np.exp(-z))\n",
    "#     s = s.reshape(s.shape[0],1)\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【GradientBoostingClassifier】\n",
      "confusion_matrix:\n",
      "[[28  8]\n",
      " [ 4 56]]\n",
      "accuracy_score:\n",
      "0.579\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.43      0.43       130\n",
      "           1       0.72      0.30      0.43        92\n",
      "           2       0.68      0.63      0.65       143\n",
      "           3       0.59      0.81      0.68       318\n",
      "           4       0.57      0.53      0.55       149\n",
      "           5       0.62      0.60      0.61       105\n",
      "           6       0.20      0.06      0.10        63\n",
      "\n",
      "    accuracy                           0.58      1000\n",
      "   macro avg       0.54      0.48      0.49      1000\n",
      "weighted avg       0.57      0.58      0.56      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                 max_depth=3, random_state=0)\n",
    "train_test(\"GradientBoostingClassifier\",clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【XGBClassifier】\n",
      "confusion_matrix:\n",
      "[[41  4]\n",
      " [ 5 60]]\n",
      "accuracy_score:\n",
      "0.63\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.46      0.46       130\n",
      "           1       0.72      0.45      0.55        92\n",
      "           2       0.68      0.68      0.68       143\n",
      "           3       0.66      0.81      0.73       318\n",
      "           4       0.62      0.61      0.61       149\n",
      "           5       0.65      0.63      0.64       105\n",
      "           6       0.53      0.30      0.38        63\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.56      0.58      1000\n",
      "weighted avg       0.63      0.63      0.62      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "train_test(\"XGBClassifier\",XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=5000,\n",
    " max_depth=4,\n",
    " min_child_weight=6,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " reg_alpha=0.005,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " seed=27), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: lamda_l2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "【LGBMClassifier】\n",
      "confusion_matrix:\n",
      "[[32  8]\n",
      " [ 4 52]]\n",
      "accuracy_score:\n",
      "0.605\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.40      0.41       130\n",
      "           1       0.71      0.35      0.47        92\n",
      "           2       0.71      0.69      0.70       143\n",
      "           3       0.60      0.81      0.69       318\n",
      "           4       0.63      0.60      0.62       149\n",
      "           5       0.62      0.57      0.60       105\n",
      "           6       0.61      0.22      0.33        63\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.61      0.52      0.54      1000\n",
      "weighted avg       0.61      0.60      0.59      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "# 构建 LGBM 分类器模型\n",
    "train_test(\"LGBMClassifier\",LGBMClassifier(\n",
    "    n_estimators=700,\n",
    "    learning_rate=0.01,\n",
    "    lambda_l1=0.6,\n",
    "    lamda_l2=0,\n",
    "    cat_smooth=1,\n",
    "    max_bin=25,\n",
    "    min_data_in_leaf=71,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_fraction=1.0,\n",
    "    bagging_freq=6,\n",
    "    num_leaves=42,\n",
    "    max_depth=10), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # GridSearch调参\n",
    "# import pandas as pd\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import GridSearchCV # Perforing grid search\n",
    "#\n",
    "# parameters = {\n",
    "#               'max_depth': [15, 20, 25, 30, 35],\n",
    "#               'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "#               'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "#               'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "#               'bagging_freq': [2, 4, 5, 6, 8],\n",
    "#               'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n",
    "#               'lambda_l2': [0, 10, 15, 35, 40],\n",
    "#               'cat_smooth': [1, 10, 15, 20, 35]\n",
    "# }\n",
    "# gbm = lgb.LGBMClassifier()\n",
    "# gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "# gsearch.fit(X_train, y_train)\n",
    "#\n",
    "# print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "# print(\"Best parameters set:\")\n",
    "# best_parameters = gsearch.best_estimator_.get_params()\n",
    "# for param_name in sorted(parameters.keys()):\n",
    "#     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxAcurate-XGB-0.63 < GCN/GAT/GraphSAGE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}